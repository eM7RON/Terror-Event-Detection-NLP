{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to pre-process pickled news articles that have been scraped from html or text documents.\n",
    "\n",
    "The environment that was used is printed out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\n",
      "# packages in environment at C:\\Anaconda3\\envs\\NLP37:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_pytorch_select           1.1.0                       cpu  \n",
      "altair                    3.1.0                    py37_0    conda-forge\n",
      "asn1crypto                0.24.0                   py37_0  \n",
      "atomicwrites              1.3.0                    py37_1  \n",
      "attrs                     19.1.0                   py37_1  \n",
      "backcall                  0.1.0                    py37_0  \n",
      "beautifulsoup4            4.7.1                    pypi_0    pypi\n",
      "blas                      1.0                         mkl  \n",
      "boto                      2.49.0                   py37_0    anaconda\n",
      "boto3                     1.9.162                    py_0    anaconda\n",
      "botocore                  1.12.163                   py_0    anaconda\n",
      "branca                    0.3.1                      py_0    conda-forge\n",
      "bs4                       0.0.1                    pypi_0    pypi\n",
      "bz2file                   0.98                     py37_1    anaconda\n",
      "ca-certificates           2019.6.16            hecc5488_0    conda-forge\n",
      "cartopy                   0.17.0          py37hfff79a3_1006    conda-forge\n",
      "certifi                   2019.6.16                py37_1    conda-forge\n",
      "cffi                      1.12.3           py37h7a1dbc1_0  \n",
      "chardet                   3.0.4                    pypi_0    pypi\n",
      "colorama                  0.4.1                    py37_0  \n",
      "corenlp-xml-reader        0.1.3                    pypi_0    pypi\n",
      "corenlpy                  0.0.6                    pypi_0    pypi\n",
      "country-list              0.1.3                    pypi_0    pypi\n",
      "cryptography              2.7              py37h7a1dbc1_0  \n",
      "cudatoolkit               10.0.130                      0  \n",
      "cycler                    0.10.0                   py37_0  \n",
      "cymem                     1.31.2           py37he980bc4_0  \n",
      "cytoolz                   0.9.0.1          py37hfa6e2cd_1  \n",
      "decorator                 4.4.0                    py37_1  \n",
      "dill                      0.2.9                    py37_0  \n",
      "docutils                  0.14                     py37_0    anaconda\n",
      "en-core-web-lg            2.0.0                    pypi_0    pypi\n",
      "en-core-web-md            2.0.0                    pypi_0    pypi\n",
      "en-core-web-sm            2.0.0                    pypi_0    pypi\n",
      "entrypoints               0.3                   py37_1000    conda-forge\n",
      "folium                    0.9.1                      py_0    conda-forge\n",
      "freetype                  2.9.1                ha9979f8_1  \n",
      "funcy                     1.12                     pypi_0    pypi\n",
      "future                    0.17.1                   py37_0  \n",
      "fuzzywuzzy                0.17.0                   pypi_0    pypi\n",
      "gensim                    3.4.0            py37hfa6e2cd_0    anaconda\n",
      "geographiclib             1.49                     pypi_0    pypi\n",
      "geopy                     1.20.0                   pypi_0    pypi\n",
      "geos                      3.7.2                habb2df7_1    conda-forge\n",
      "icc_rt                    2019.0.0             h0cc432a_1  \n",
      "icu                       58.2                 ha66f8fd_1  \n",
      "idna                      2.8                      pypi_0    pypi\n",
      "imbalanced-learn          0.5.0                    pypi_0    pypi\n",
      "importlib_metadata        0.17                     py37_1  \n",
      "intel-openmp              2019.4                      245  \n",
      "ipykernel                 5.1.1            py37h39e3cac_0  \n",
      "ipython                   7.6.0            py37h39e3cac_0  \n",
      "ipython_genutils          0.2.0                    py37_0  \n",
      "jedi                      0.13.3                   py37_0  \n",
      "jinja2                    2.10.1                   py37_0  \n",
      "jmespath                  0.9.4                      py_0    anaconda\n",
      "joblib                    0.13.2                   py37_0  \n",
      "jpeg                      9b                   hb83a4c4_2  \n",
      "jsonschema                3.0.1                    py37_0    conda-forge\n",
      "jupyter_client            5.2.4                    py37_0  \n",
      "jupyter_core              4.4.0                    py37_0  \n",
      "kiwisolver                1.1.0            py37ha925a31_0  \n",
      "libpng                    1.6.37               h2a8f88b_0  \n",
      "libsodium                 1.0.16               h9d3ae62_0  \n",
      "libtiff                   4.0.10               hb898794_2  \n",
      "m2w64-gcc-libgfortran     5.3.0                         6  \n",
      "m2w64-gcc-libs            5.3.0                         7  \n",
      "m2w64-gcc-libs-core       5.3.0                         7  \n",
      "m2w64-gmp                 6.1.0                         2  \n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2  \n",
      "markupsafe                1.1.1            py37he774522_0  \n",
      "matplotlib                3.1.0            py37hc8f65d3_0  \n",
      "mkl                       2019.4                      245  \n",
      "mkl-service               2.0.2            py37he774522_0  \n",
      "mkl_fft                   1.0.12           py37h14836fe_0  \n",
      "mkl_random                1.0.2            py37h343c172_0  \n",
      "more-itertools            7.0.0                    py37_0  \n",
      "msgpack-numpy             0.4.4.2                  py37_0  \n",
      "msgpack-python            0.5.6            py37he980bc4_1  \n",
      "msys2-conda-epoch         20160418                      1  \n",
      "murmurhash                0.28.0           py37he025d50_0  \n",
      "networkx                  2.3                        py_0  \n",
      "ninja                     1.9.0            py37h74a9793_0  \n",
      "nltk                      3.4.1                    py37_0  \n",
      "numexpr                   2.6.9            py37hdce8814_0  \n",
      "numpy                     1.16.4           py37h19fb1c0_0  \n",
      "numpy-base                1.16.4           py37hc3f5095_0  \n",
      "olefile                   0.46                     py37_0  \n",
      "openssl                   1.1.1c               hfa6e2cd_0    conda-forge\n",
      "owslib                    0.18.0                     py_0    conda-forge\n",
      "packaging                 19.0                     py37_0  \n",
      "pandas                    0.24.2           py37ha925a31_0  \n",
      "parso                     0.4.0                      py_0  \n",
      "pickleshare               0.7.5                    py37_0  \n",
      "pillow                    6.0.0            py37hdc69c19_0  \n",
      "pip                       19.1.1                   py37_0  \n",
      "plac                      0.9.6                    py37_0  \n",
      "pluggy                    0.12.0                     py_0  \n",
      "preshed                   1.0.1            py37he025d50_0  \n",
      "proj4                     5.2.0             h6538335_1004    conda-forge\n",
      "prompt_toolkit            2.0.9                    py37_0  \n",
      "protobuf                  3.8.0                    pypi_0    pypi\n",
      "py                        1.8.0                    py37_0  \n",
      "pycorenlp                 0.3.0                    pypi_0    pypi\n",
      "pycountry                 19.7.15                  pypi_0    pypi\n",
      "pycparser                 2.19                     py37_0  \n",
      "pyepsg                    0.4.0                      py_0    conda-forge\n",
      "pygments                  2.4.2                      py_0  \n",
      "pykdtree                  1.3.1           py37h452e1ab_1002    conda-forge\n",
      "pyldavis                  2.1.2                    pypi_0    pypi\n",
      "pyopenssl                 19.0.0                   py37_0  \n",
      "pyparsing                 2.4.0                      py_0  \n",
      "pyproj                    1.9.6           py37hfa6e2cd_1002    conda-forge\n",
      "pyqt                      5.9.2            py37h6538335_2  \n",
      "pyreadline                2.1                      py37_1  \n",
      "pyrsistent                0.15.3           py37hfa6e2cd_0    conda-forge\n",
      "pyshp                     2.1.0                      py_0    conda-forge\n",
      "pysocks                   1.7.0                    py37_0  \n",
      "pytest                    5.0.1                    py37_0  \n",
      "python                    3.7.1                h8c8aaf0_6  \n",
      "python-dateutil           2.8.0                    py37_0  \n",
      "python-igraph             0.7.1.post6              pypi_0    pypi\n",
      "python-levenshtein        0.12.0          py37hfa6e2cd_1001    conda-forge\n",
      "python-louvain            0.13                       py_0  \n",
      "pytorch                   1.1.0           py3.7_cuda100_cudnn7_1    pytorch\n",
      "pytz                      2019.1                     py_0  \n",
      "pyzmq                     18.0.0           py37ha925a31_0  \n",
      "qt                        5.9.7            vc14h73c81de_0  \n",
      "regex                     2018.07.11       py37hfa6e2cd_0  \n",
      "requests                  2.22.0                   py37_0  \n",
      "s3transfer                0.2.0                    py37_0    anaconda\n",
      "scikit-learn              0.21.2           py37h6288b17_0  \n",
      "scipy                     1.2.1            py37h29ff71c_0    anaconda\n",
      "selenium                  3.141.0          py37he774522_0  \n",
      "setuptools                41.0.1                   py37_0  \n",
      "shapely                   1.6.4           py37ha35856d_1006    conda-forge\n",
      "sip                       4.19.8           py37h6538335_0  \n",
      "six                       1.12.0                   py37_0  \n",
      "smart_open                1.8.4                      py_0    anaconda\n",
      "sner                      0.2.12                   pypi_0    pypi\n",
      "soupsieve                 1.9.2                    pypi_0    pypi\n",
      "spacy                     2.0.12           py37h8300f20_0  \n",
      "sqlite                    3.28.0               he774522_0  \n",
      "stanfordnlp               0.2.0                    pypi_0    pypi\n",
      "termcolor                 1.1.0                    py37_1  \n",
      "thinc                     6.10.3           py37h8300f20_0  \n",
      "tk                        8.6.8                hfa6e2cd_0  \n",
      "toolz                     0.9.0                    py37_0  \n",
      "torchvision               0.3.0              py37_cu100_1    pytorch\n",
      "tornado                   6.0.2            py37he774522_0  \n",
      "tqdm                      4.32.2                   pypi_0    pypi\n",
      "traitlets                 4.3.2                    py37_0  \n",
      "ujson                     1.35             py37hfa6e2cd_0  \n",
      "urllib3                   1.25.3                   pypi_0    pypi\n",
      "vc                        14.1                 h0510ff6_4  \n",
      "vincent                   0.4.4                      py_1    conda-forge\n",
      "vs2015_runtime            14.15.26706          h3a45250_4  \n",
      "wcwidth                   0.1.7                    py37_0  \n",
      "wheel                     0.33.4                   py37_0  \n",
      "win_inet_pton             1.1.0                    py37_0  \n",
      "wincertstore              0.2                      py37_0  \n",
      "word2vec                  0.9.4            py37h452e1ab_0  \n",
      "wordcloud                 1.5.0           py37hfa6e2cd_1000    conda-forge\n",
      "wrapt                     1.10.11          py37hfa6e2cd_2  \n",
      "xlrd                      1.2.0                    py37_0  \n",
      "xz                        5.2.4                h2fa13f4_4  \n",
      "zeromq                    4.3.1                h33f27b4_3  \n",
      "zipp                      0.5.1                      py_0  \n",
      "zlib                      1.2.11               h62dcd97_3  \n",
      "zstd                      1.3.7                h508b16e_0  \n"
     ]
    }
   ],
   "source": [
    "print(__import__('sys').version)\n",
    "!conda list -n NLP37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import tz\n",
    "from datetime import date, datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "from toolz import compose, curry, concat\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "\n",
    "import os, time, re, pytz\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DOC_SIZE = 20 # minimum size of a document for filtering\n",
    "\n",
    "###########################################################################\n",
    "#        Some regex pattern for removing invalid unicode characters       #\n",
    "###########################################################################\n",
    "\n",
    "NORMALIZATION_PROTOCOL = 'NFKD' # Alt: NFKD, NFKC, NFC, NFD\n",
    "\n",
    "CTRL_REGEX = re.compile(r'[\\x00-\\x1f\\x7f-\\x9f]|\\s{2,}') # Alt: #r'[^\\x00-\\x7f]'\n",
    "                                                        # replace 2 or more space: \\s{2,}\n",
    "    \n",
    "WORD_REGEX = re.compile(r'[\\w\\']+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(filename, data, protocol=2):\n",
    "    with open(os.path.normpath(filename), 'wb') as open_file:\n",
    "        pickle.dump(data, open_file, protocol=protocol)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(os.path.normpath(filename), 'rb') as open_file:\n",
    "        return pickle.load(open_file)\n",
    "\n",
    "def replace_bad_chars(text, regex=CTRL_REGEX):\n",
    "    '''\n",
    "    Removes all ctrl characters such as '\\n'\n",
    "    and consecutive spaces\n",
    "    '''\n",
    "    return regex.sub(r' ', text)\n",
    "\n",
    "def split_word_boundary(text, regex=WORD_REGEX):\n",
    "    return regex.findall(text)\n",
    "\n",
    "def normalize_unicode(text, protocol=NORMALIZATION_PROTOCOL):\n",
    "    '''\n",
    "    Converts all text to ascii encoding and then back to utf-8 in order\n",
    "    to remove invalid characters for nlp.\n",
    "    '''\n",
    "    return normalize(protocol, text).encode('ascii', 'ignore').decode('utf8')\n",
    "\n",
    "clean_text = compose(lambda x: x.replace('&', 'and'),\n",
    "                     replace_bad_chars,\n",
    "                     normalize_unicode)\n",
    "\n",
    "def map_clean_text(data):\n",
    "    print('Cleaning text...')\n",
    "    progress = tqdm(range(len(data)))\n",
    "    for doc in data:\n",
    "        doc['content'] = clean_text(doc['content'])\n",
    "        progress.update(1)\n",
    "    return data\n",
    "\n",
    "def stamp_paragraph_breaks(data):\n",
    "    print('Stamping paragraphs...')\n",
    "    progress = tqdm(range(len(data)))\n",
    "    for doc in data:\n",
    "        doc['content'] = doc['content'].replace('\\n', ' NEWPARAGRAPH ').replace('  ', ' ')\n",
    "        progress.update(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_explorer(extension, directory):\n",
    "    '''\n",
    "    A generator to find filenames with a given extension within a given \n",
    "    directory\n",
    "    '''\n",
    "    ext_upper, ext_lower = extension.lower(), extension.upper()\n",
    "    for filename in os.listdir(os.path.normpath(directory)):\n",
    "        if filename.endswith(ext_upper) \\\n",
    "        or filename.endswith(ext_lower):\n",
    "            yield '%s/%s' % (directory, filename)\n",
    "\n",
    "def aggregate_docs(filenames):\n",
    "    output = []\n",
    "    for filename in filenames:\n",
    "        output.extend(load_pickle(filename))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_by_date(lis):\n",
    "    output = {}\n",
    "    for doc in lis:\n",
    "        date = doc['datetime'].date()\n",
    "        if date in output:\n",
    "            output[date].append(doc)\n",
    "        else:\n",
    "            output[date] = [doc]\n",
    "    return [output[date] for date in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_brackets(window_size, step, start='01 Jan 2017 00:00', end='05 Jan 2018 00:00'):\n",
    "    '''\n",
    "    Creates a list of tuples where each tuple contains two datetime\n",
    "    objects a 'window_size' apart. Each tuple will be a step apart\n",
    "    i.e. No. tuples = (start - end) / step\n",
    "    '''\n",
    "    start = datetime.strptime(start, '%d %b %Y %H:%M').replace(tzinfo=pytz.utc)\n",
    "    end = datetime.strptime(end, '%d %b %Y %H:%M').replace(tzinfo=pytz.utc)\n",
    "    window_size = timedelta(hours=window_size)\n",
    "    step = timedelta(hours=step)\n",
    "    brackets = []\n",
    "    \n",
    "    while start < end:\n",
    "        brackets.append((start, start + window_size))\n",
    "        start += step\n",
    "        \n",
    "    return brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter documents which are too short and have no useful information\n",
    "filter_length = curry(filter)(lambda x: len(x['content']) > MIN_DOC_SIZE)\n",
    "\n",
    "def datetime_sort(data):\n",
    "    '''\n",
    "    Sort documents based on datetime\n",
    "    '''\n",
    "    return sorted(data, key=lambda x: x['datetime'])\n",
    "\n",
    "def datetime_array(lis):\n",
    "    '''\n",
    "    Creates an array of datetime objects from the corpus\n",
    "    '''\n",
    "    return array([doc['datetime'] for doc in lis])\n",
    "\n",
    "def assign_ids(data):\n",
    "    for i, doc in enumerate(data):\n",
    "        doc['id'] = i\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gtd_info(data):\n",
    "    regex = re.compile(r'   Incident Summary:   \"?\\d{2}/\\d{2}/\\d{4}: ')\n",
    "    return [regex.sub('', info).strip() for info in tqdm(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_correction(data):\n",
    "    '''\n",
    "    Some docs seem to be missing datetime which is because the date seems to have \n",
    "    been collected in the content on some docs. This function will correct this.\n",
    "    '''\n",
    "    \n",
    "    print('Correcting datetime captures...')\n",
    "    def datetime_interpretor(*args, default_tzinfo=tz.gettz('UTC'), **kwargs):\n",
    "        dt = parse(*args, **kwargs)[0]\n",
    "        return dt.replace(tzinfo=dt.tzinfo or default_tzinfo)\n",
    "    \n",
    "    custom_tz_path = os.path.normpath(\n",
    "    r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Timezones__/timezones.pkl')\n",
    "\n",
    "    timezones = {t: pytz.timezone(t) for t in pytz.all_timezones}\n",
    "\n",
    "    if custom_tz_path:\n",
    "        with open(custom_tz_path, 'rb') as file:\n",
    "            custom_tz = pickle.load(file)\n",
    "            for tz_code in custom_tz.keys():\n",
    "                if tz_code not in timezones:\n",
    "                    timezones[tz_code] = custom_tz[tz_code]\n",
    "                    \n",
    "    deletes = []\n",
    "\n",
    "    for i, doc in enumerate(data):\n",
    "        if type(doc['datetime']) == str:\n",
    "            try:\n",
    "                content = doc['content'].split(' ')\n",
    "                date = ' '.join(content[:3])\n",
    "                doc['raw-date'] = date\n",
    "                doc['datetime'] = datetime_interpretor(date, \n",
    "                                                    fuzzy_with_tokens=True, \n",
    "                                                    tzinfos=timezones\n",
    "                                                    ).astimezone(pytz.timezone('utc'))\n",
    "                doc['content'] = ' '.join(content[3:])\n",
    "            except ValueError:\n",
    "                deletes.append(i)\n",
    "    print('%s docs removed due to invalid dates' % len(deletes))\n",
    "    for i in sorted(deletes, reverse=True):\n",
    "        del data[i]\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publication_correction(data):\n",
    "    print('Correcting publication captures...')\n",
    "    for doc in data:\n",
    "        if doc['publication'] == 'Unknown':\n",
    "            content = doc['content'].split('. ')\n",
    "            if content[0].startswith('The Guardian'):\n",
    "                doc['publication'] = content[0].strip()\n",
    "                doc['content'] = '. '.join(content[1: ])\n",
    "            elif doc['content'][:55] == 'Supplied by BBC Worldwide Monitoring. By BBC Monitoring':\n",
    "                doc['publication'] = 'BBC Monitoring'\n",
    "                doc['content'] = doc['content'][55:]\n",
    "            elif doc['content'][:17] == 'By BBC Monitoring':\n",
    "                doc['publication'] = 'BBC Monitoring'\n",
    "                doc['content'] = doc['content'][17:]    \n",
    "            elif doc['content'][:36] == 'Supplied by BBC Worldwide Monitoring':\n",
    "                doc['publication'] = 'BBC Monitoring'\n",
    "                doc['content'] = doc['content'][36:]\n",
    "        if 'The Guardian' in doc['publication']:\n",
    "            doc['publication'] = 'The Guardian'\n",
    "        elif 'BBC Monitoring' in doc['publication'] \\\n",
    "        or 'Supplied by BBC' in doc['publication']:\n",
    "            doc['publication'] = 'BBC Monitoring'\n",
    "        elif doc['publication'] == 'EuroNews - English Version':\n",
    "            doc['publication'] = 'EuroNews'\n",
    "        elif doc['publication'] == 'Ukrinform(Ukraine)':\n",
    "            doc['publication'] = 'Ukrinform'\n",
    "        elif doc['publication'] == 'Agence France Presse -- English':\n",
    "            doc['publication'] = 'Agence France Presse'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dedupe(brackets, lis):\n",
    "    '''\n",
    "    Moves a sliding window accross the dataset and removes documents with \n",
    "    identical content within the window. The oldest document will be\n",
    "    retained in each case.\n",
    "    '''\n",
    "    print('Removing duplicate documents... ')\n",
    "    start_size = len(lis)\n",
    "    progress = tqdm(range(len(brackets)))\n",
    "    for start, end in brackets:\n",
    "        \n",
    "        dt_array = array([doc['datetime'] for doc in lis])\n",
    "        keeps = []\n",
    "        idx = where((start <= dt_array) & (dt_array < end))[0]\n",
    "\n",
    "        if len(idx) > 0:\n",
    "            idx_range = range(len(idx))\n",
    "            deletes = set()\n",
    "            for i in idx_range:\n",
    "                for j in idx_range[i + 1:]:\n",
    "                    if lis[idx[i]]['content'] == lis[idx[j]]['content']:\n",
    "\n",
    "                        deletes.add(idx[j])\n",
    "\n",
    "            deletes = sorted(deletes, reverse=True)\n",
    "            for i in deletes:\n",
    "                del lis[i]\n",
    "        \n",
    "        progress.update(1)\n",
    "            \n",
    "    end_size = len(lis)\n",
    "    \n",
    "    print('removed %d docs' % (start_size - end_size))\n",
    "    \n",
    "    return lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dechild(brackets, lis, datetime_array=datetime_array):\n",
    "    '''\n",
    "    Moves a sliding window accross the dataset and removes documents which \n",
    "    are children i.e. their content is identical to a substring in another\n",
    "    (parent) document within the window. The most unique document will be\n",
    "    retained.\n",
    "    '''\n",
    "    print('Removing child documents... ')\n",
    "    start_size = len(lis)\n",
    "    progress = tqdm(range(len(brackets)))\n",
    "    for start, end in brackets:\n",
    "        \n",
    "        dt_array = array([doc['datetime'] for doc in lis])\n",
    "        keeps = []\n",
    "        idx = where((start <= dt_array) & (dt_array < end))[0]\n",
    "        idx_range = range(len(idx))\n",
    "        deletes = set()\n",
    "        for i in idx_range:\n",
    "            for j in idx_range[i + 1:]:\n",
    "                if lis[idx[i]]['content'] in lis[idx[j]]['content']:\n",
    "                    deletes.add(idx[j])\n",
    "                elif lis[idx[j]]['content'] in lis[idx[i]]['content']:\n",
    "                    deletes.add(idx[i])\n",
    "                    \n",
    "        deletes = sorted(deletes, reverse=True)\n",
    "        for i in deletes:\n",
    "            del lis[i]\n",
    "        \n",
    "        progress.update(1)\n",
    "            \n",
    "    end_size = len(lis)\n",
    "    \n",
    "    print('removed %d docs' % (start_size - end_size))\n",
    "    \n",
    "    return lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a pipe and make sure the input and output paths are correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\02 HTML Scrapes\\out\\EuroNews_-_English_Version_Sky_News_Ukrinfor2019-07-18_03-41.pkl'\n",
    "OUTPUT_FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\03 Preprocessing\\out\\pre_processed.pkl'\n",
    "\n",
    "preprocess_pipe = compose(curry(save_pickle)(OUTPUT_FILENAME),\n",
    "                          assign_ids,\n",
    "                          curry(windowed_dechild)(window_brackets(48, 12)),\n",
    "                          curry(windowed_dedupe)(window_brackets(48, 12)),\n",
    "                          datetime_sort,\n",
    "                          filter_length,\n",
    "                          datetime_correction,\n",
    "                          publication_correction,\n",
    "                          map_clean_text,\n",
    "                          load_pickle,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 236/236 [00:00<00:00, 1903.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting publication captures...\n",
      "Correcting datetime captures...\n",
      "0 docs removed due to invalid dates\n",
      "Removing duplicate documents... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████▊| 736/738 [00:00<00:00, 931.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 8 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 738/738 [00:00<00:00, 1033.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing child documents... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████▉        | 663/738 [00:01<00:00, 108.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 1 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 738/738 [00:01<00:00, 430.57it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_pipe(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pickle(r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\03 Preprocessing\\out\\pre_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\03 Preprocessing\\Raw Scrape.pkl'\n",
    "OUTPUT_FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\03 Preprocessing\\Paragraphs.pkl'\n",
    "\n",
    "preprocess_pipe = compose(\n",
    "                          curry(save_pickle)(OUTPUT_FILENAME),\n",
    "                          assign_ids,\n",
    "                          curry(windowed_dechild)(window_brackets(48, 12)),\n",
    "                          curry(windowed_dedupe)(window_brackets(48, 12)),\n",
    "                          datetime_sort,\n",
    "                          filter_length,\n",
    "                          datetime_correction,\n",
    "                          publication_correction,\n",
    "                          map_clean_text,\n",
    "                          #stamp_paragraph_breaks,\n",
    "                          load_pickle,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipe(FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process SVM training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6829/6829 [00:01<00:00, 3642.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correcting publication captures...\n",
      "Correcting datetime captures...\n",
      "27 docs removed due to invalid dates\n",
      "Removing duplicate documents... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████▋| 3278/3291 [00:29<00:00, 154.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 2232 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3291/3291 [00:40<00:00, 154.53it/s]"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\04 Training Data\\SVM Negatives'\n",
    "INPUT_FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\04 Training Data\\SVM Negatives\\Agg.pkl'\n",
    "OUTPUT_FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\04 Training Data\\SVM Negatives\\Agg2.pkl'\n",
    "\n",
    "pipe = compose(curry(save_pickle)(OUTPUT_FILENAME),\n",
    "               curry(windowed_dedupe)(window_brackets(48, 24, '01 Jan 2009 00:00')),\n",
    "               datetime_correction,\n",
    "               publication_correction,\n",
    "               map_clean_text,\n",
    "               #load_pickle,\n",
    "               aggregate_docs,\n",
    "               curry(directory_explorer)('.pkl'),\n",
    "               )\n",
    "\n",
    "pipe(INPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process GTD descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\GTD\\Info.pkl'\n",
    "OUTPUT_FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\GTD\\Preprocessed Info.pkl'\n",
    "\n",
    "\n",
    "process_gtd_info = compose(curry(save_pickle)(OUTPUT_FILENAME),\n",
    "                           clean_gtd_info,\n",
    "                           curry(map)(clean_text),\n",
    "                           load_pickle,\n",
    "                           )\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_gtd_info(INPUT_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
