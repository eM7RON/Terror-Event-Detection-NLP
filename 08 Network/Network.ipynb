{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below environment is required for this notebook to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__import__('sys').version)\n",
    "!conda list -n NLP37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.lfd.uci.edu/~gohlke/pythonlibs/#python-igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab qt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from toolz import curry, compose\n",
    "from itertools import repeat\n",
    "from types import FunctionType\n",
    "from itertools import combinations\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "from dateutil.tz import tz\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pytz\n",
    "import community\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(filename, data):\n",
    "    print('Pickling data...')\n",
    "    with open(os.path.normpath(filename), 'wb') as open_file:\n",
    "        pickle.dump(data, open_file)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    print('Loading pickled data...')\n",
    "    with open(os.path.normpath(filename), 'rb') as open_file:\n",
    "        return pickle.load(open_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_input(data):\n",
    "    output = {}\n",
    "    for doc in data:\n",
    "        date = doc['datetime']\n",
    "        if date not in output:\n",
    "            output[date] = {'docs': []}\n",
    "        output[date]['docs'].append(doc)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_degree(data):\n",
    "    \n",
    "    for date in data:\n",
    "        for doc in data[date]['docs']:\n",
    "            for edge in doc['edges']:\n",
    "                for node in edge['nodes']:\n",
    "                    data[date]['nodes'][node] += edge['weight']\n",
    "                \n",
    "    return data\n",
    "            \n",
    "\n",
    "def sentence_significance(omissions, data):\n",
    "    \n",
    "    for date in data:\n",
    "        for doc in data[date]['docs']:\n",
    "            \n",
    "            nodes = {}\n",
    "            edges = []\n",
    "            doc_ents = []\n",
    "            \n",
    "            for sentence in doc['sentences']:\n",
    "                doc_ents.extend(sentence)\n",
    "                \n",
    "            doc_ents = {ent['word'] for ent in doc_ents if ent['tag'] not in omissions}\n",
    "            \n",
    "            for ent in doc_ents:\n",
    "                if ent not in nodes:\n",
    "                    nodes[ent] = {'weight': 0, 'neighbours': {}}\n",
    "            \n",
    "            for sentence in doc['sentences']:\n",
    "\n",
    "                counts = entity_counter(sentence, omissions)\n",
    "                sig, n_ent = compute_significance(counts)\n",
    "\n",
    "                ents = combinations(list(sig.keys()), 2)\n",
    "\n",
    "                for ent1, ent2 in ents:\n",
    "                    \n",
    "                    if ent1 != ent2:\n",
    "\n",
    "                        if ent2 in nodes[ent1]['neighbours']:\n",
    "                            nodes[ent1]['neighbours'][ent2]['weight'] += sig[ent1] + sig[ent2]\n",
    "                        else:\n",
    "                            edge = {'weight': sig[ent1] + sig[ent2], 'nodes': [ent1, ent2]}\n",
    "                            nodes[ent1]['neighbours'][ent2] = edge\n",
    "                            nodes[ent2]['neighbours'][ent1] = edge\n",
    "                            edges.append(edge)\n",
    "                        \n",
    "            doc['nodes'] = nodes\n",
    "            doc['edges'] = edges\n",
    "    return data\n",
    "\n",
    "def document_significance(omissions, data):\n",
    "    \n",
    "    for date in data:\n",
    "        for doc in data[date]['docs']:\n",
    "            \n",
    "            doc_ents = []\n",
    "\n",
    "            for sentence in doc['sentences']:\n",
    "                doc_ents.extend(sentence)\n",
    "                \n",
    "            if 'nodes' not in doc:\n",
    "                doc['nodes'] = {}\n",
    "                doc['edges'] = []\n",
    "\n",
    "            counts = entity_counter(doc_ents, omissions)\n",
    "            sig, n_ent = compute_significance(counts)\n",
    "            \n",
    "            doc_ents = {ent['word'] for ent in doc_ents if ent['tag'] not in omissions}\n",
    "\n",
    "            for ent in doc_ents:\n",
    "                if ent not in doc['nodes']:\n",
    "                    doc['nodes'][ent] = {'weight': 0, 'neighbours': {}}\n",
    "\n",
    "            ents = combinations(doc_ents, 2)\n",
    "\n",
    "            for ent1, ent2 in ents:\n",
    "                    \n",
    "                if ent1 in sig:\n",
    "                    n1 = sig[ent1]\n",
    "                else:\n",
    "                    n1 = 0\n",
    "                if ent2 in sig:\n",
    "                    n2 = sig[ent2]\n",
    "                else:\n",
    "                    n2 = 0\n",
    "\n",
    "                if ent2 in doc['nodes'][ent1]['neighbours']:\n",
    "                    doc['nodes'][ent1]['neighbours'][ent2]['weight'] += n1 + n2\n",
    "                else:\n",
    "                    edge = {'weight': n1 + n2, 'nodes': [ent1, ent2]}\n",
    "                    doc['nodes'][ent1]['neighbours'][ent2] = edge\n",
    "                    doc['nodes'][ent2]['neighbours'][ent1] = edge\n",
    "                    doc['edges'].append(edge)\n",
    "    return data\n",
    "\n",
    "def sentence_co_occurrence(omissions, data):\n",
    "    \n",
    "    for date in data:\n",
    "        \n",
    "        for doc in data[date]['docs']:\n",
    "            \n",
    "            doc['nodes'] = {}\n",
    "            doc['edges'] = []\n",
    "            \n",
    "            for sentence in doc['sentences']:\n",
    "                \n",
    "                sentence = [ent['word'] for ent in sentence if ent['tag'] not in omissions]\n",
    "\n",
    "                for ent in set(sentence):\n",
    "                    if ent not in doc['nodes']:\n",
    "                        doc['nodes'][ent] = {'weight': 0, 'neighbours': {}}\n",
    "\n",
    "                ents = combinations(sentence, 2)\n",
    "\n",
    "                for ent1, ent2 in ents:\n",
    "                    if ent1 != ent2:\n",
    "                        if ent2 in doc['nodes'][ent1]['neighbours']:\n",
    "                            doc['nodes'][ent1]['neighbours'][ent2]['weight'] += 1\n",
    "                        else:\n",
    "                            edge = {'weight': 1, 'nodes': [ent1, ent2]}\n",
    "                            doc['nodes'][ent1]['neighbours'][ent2] = edge\n",
    "                            doc['nodes'][ent2]['neighbours'][ent1] = edge\n",
    "                            doc['edges'].append(edge) \n",
    "    return data\n",
    "\n",
    "def document_co_occurrence(omissions, data):\n",
    "    \n",
    "    for date in data:\n",
    "        \n",
    "        for doc in data[date]['docs']:\n",
    "            \n",
    "            doc_ents = []\n",
    "            \n",
    "            if 'nodes' not in doc:\n",
    "                doc['nodes'] = {}\n",
    "                doc['edges'] = []\n",
    "                \n",
    "            for sentence in doc['sentences']:\n",
    "                doc_ents.extend(sentence)\n",
    "                \n",
    "            doc_ents = [ent['word'] for ent in doc_ents if ent['tag'] not in omissions]\n",
    "\n",
    "            for ent in set(doc_ents):\n",
    "                if ent not in doc['nodes']:\n",
    "                    doc['nodes'][ent] = {'weight': 0, 'neighbours': {}}\n",
    "\n",
    "            ents = combinations(doc_ents, 2)\n",
    "\n",
    "            for ent1, ent2 in ents:\n",
    "                if ent1 != ent2:\n",
    "                    if ent2 in doc['nodes'][ent1]['neighbours']:\n",
    "                        doc['nodes'][ent1]['neighbours'][ent2]['weight'] += 1\n",
    "                    else:\n",
    "                        edge = {'weight': 1, 'nodes': [ent1, ent2]}\n",
    "                        doc['nodes'][ent1]['neighbours'][ent2] = edge\n",
    "                        doc['nodes'][ent2]['neighbours'][ent1] = edge\n",
    "                        doc['edges'].append(edge) \n",
    "    return data\n",
    "\n",
    "def compute_significance(counter):\n",
    "    sig = {}\n",
    "    n_ent = sum(list(counter.values()))\n",
    "    for entity in counter:\n",
    "        sig[entity] = counter[entity] / n_ent\n",
    "    return sig, n_ent\n",
    "\n",
    "def entity_counter(ents, omissions):\n",
    "    counter = {}\n",
    "    for ent in ents:\n",
    "        if ent['tag'] not in omissions:\n",
    "            if ent['word'] in counter:\n",
    "                counter[ent['word']] += 1\n",
    "            else:\n",
    "                counter[ent['word']] = 1\n",
    "    return counter\n",
    "\n",
    "def document_entities(data):\n",
    "    \n",
    "    for date in data:\n",
    "        for doc in data[date]['docs']:\n",
    "            doc['ents'] = []\n",
    "            for sentence in doc['sentences']:\n",
    "                for ent in sentence:\n",
    "                    doc['ents'].append(ent)\n",
    "    return data\n",
    "                    \n",
    "\n",
    "def aggregate_dictionary_nodes_and_edges(data):\n",
    "    \n",
    "    for date in data:\n",
    "        nodes, edges = {}, []\n",
    "        for doc in data[date]['docs']:\n",
    "            for node in doc['nodes']:\n",
    "                nodes[node] = 0\n",
    "            edges.extend(doc['edges'])\n",
    "        data[date]['nodes'] = nodes\n",
    "        data[date]['edges'] = edges\n",
    "\n",
    "    return data\n",
    "\n",
    "def list_data_structure(data):\n",
    "    output = []\n",
    "    for date in data:\n",
    "        for doc in data[date]['docs']:\n",
    "            output.append(doc)\n",
    "    return output\n",
    "\n",
    "def strip_content(data):\n",
    "    for date in data:\n",
    "        docs = []\n",
    "        for doc in data[date]['docs']:\n",
    "            del doc['sentences']\n",
    "            docs.append(doc)\n",
    "            \n",
    "        data[date]['docs'] = docs\n",
    "    return data\n",
    "\n",
    "def filter_dates(data, start='31 Dec 2016', end='01 Jan 2018'):\n",
    "    \n",
    "    start = dt.strptime(start, '%d %b %Y')\n",
    "    end = dt.strptime(end, '%d %b %Y')\n",
    "    \n",
    "    output = {}\n",
    "    \n",
    "    for date in data:\n",
    "        if start < date < end:\n",
    "            output[date] = data[date]\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'demo\\output\\person'\n",
    "\n",
    "fnames = [\n",
    "         r'\\01 sent sig',\n",
    "         r'\\02 doc sig',\n",
    "         r'\\03 doc sent sig',\n",
    "         r'\\04 sent count',\n",
    "         r'\\05 doc count',\n",
    "         r'\\06 doc sent count',\n",
    "         ]\n",
    "\n",
    "omitted_tags = set(['location', 'organization'])\n",
    "\n",
    "funcs = [\n",
    "        compose(calculate_node_degree,\n",
    "                aggregate_dictionary_nodes_and_edges,\n",
    "                #curry(document_significance)(omitted_tags),\n",
    "                curry(sentence_significance)(omitted_tags),\n",
    "                #curry(document_co_occurrence)(omitted_tags),\n",
    "                #curry(sentence_co_occurrence)(omitted_tags),\n",
    "                filter_dates,\n",
    "                unpack_input,\n",
    "               ),\n",
    "\n",
    "        compose(calculate_node_degree,\n",
    "                       aggregate_dictionary_nodes_and_edges,\n",
    "                       curry(document_significance)(omitted_tags),\n",
    "                       #curry(sentence_significance)(omitted_tags),\n",
    "                       #curry(document_co_occurrence)(omitted_tags),\n",
    "                       #curry(sentence_co_occurrence)(omitted_tags),\n",
    "                       filter_dates,\n",
    "                       unpack_input,\n",
    "               ),\n",
    "\n",
    "        compose(calculate_node_degree,\n",
    "                       aggregate_dictionary_nodes_and_edges,\n",
    "                       curry(document_significance)(omitted_tags),\n",
    "                       curry(sentence_significance)(omitted_tags),\n",
    "                       #curry(document_co_occurrence)(omitted_tags),\n",
    "                       #curry(sentence_co_occurrence)(omitted_tags),\n",
    "                       filter_dates,\n",
    "                       unpack_input,\n",
    "               ),\n",
    "\n",
    "        compose(calculate_node_degree,\n",
    "                aggregate_dictionary_nodes_and_edges,\n",
    "                #curry(document_significance)(omitted_tags),\n",
    "                #curry(sentence_significance)(omitted_tags),\n",
    "                #curry(document_co_occurrence)(omitted_tags),\n",
    "                curry(sentence_co_occurrence)(omitted_tags),\n",
    "                filter_dates,\n",
    "                unpack_input,\n",
    "               ),\n",
    "\n",
    "        compose(calculate_node_degree,\n",
    "                aggregate_dictionary_nodes_and_edges,\n",
    "                #curry(document_significance)(omitted_tags),\n",
    "                #curry(sentence_significance)(omitted_tags),\n",
    "                curry(document_co_occurrence)(omitted_tags),\n",
    "                #curry(sentence_co_occurrence)(omitted_tags),\n",
    "                filter_dates,\n",
    "                unpack_input,\n",
    "               ),\n",
    "\n",
    "        compose(calculate_node_degree,\n",
    "                aggregate_dictionary_nodes_and_edges,\n",
    "                #curry(document_significance)(omitted_tags),\n",
    "                #curry(sentence_significance)(omitted_tags),\n",
    "                curry(document_co_occurrence)(omitted_tags),\n",
    "                curry(sentence_co_occurrence)(omitted_tags),\n",
    "                filter_dates,\n",
    "                unpack_input,\n",
    "               ),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell will generate graphs using Python data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Loading pickled data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Loading pickled data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Loading pickled data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Loading pickled data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Loading pickled data...\n",
      "Pickling data...\n",
      "Pickling data...\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(fnames):\n",
    "    data = load_pickle(r'data\\wiki.pkl')\n",
    "    data = funcs[i](data)\n",
    "    save_pickle(folder + r'\\graphs' + f + r' dic.pkl', data)\n",
    "    lis = list_data_structure(data)\n",
    "    del data\n",
    "    save_pickle(folder + r'\\graphs' + f + r' list.pkl', lis)\n",
    "    del lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle(r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\08 Network\\demo\\output\\person\\graphs\\02 doc sig dic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networkx graph stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data, use_node_weight=True):\n",
    "    print('Building graph...')\n",
    "    graph = nx.Graph()\n",
    "    for node in data['nodes']:\n",
    "        if use_node_weight:\n",
    "            if node not in graph:\n",
    "                graph.add_node(node, weight=data['nodes'][node]['weight'])\n",
    "                graph.node[node]['viz'] = {'size': data['nodes'][node]['weight']}\n",
    "            else:\n",
    "                graph.node[node]['weight'] += data['nodes'][node]['weight']\n",
    "                graph.node[node]['viz']['size'] += data['nodes'][node]['weight']\n",
    "        else:\n",
    "            if node not in graph:\n",
    "                graph.add_node(node)\n",
    "                \n",
    "    for edge in tqdm(data['edges']):\n",
    "        if graph.has_edge(*edge['nodes']):\n",
    "            graph[edge['nodes'][0]][edge['nodes'][1]]['weight'] += edge['weight']\n",
    "        else:\n",
    "            graph.add_edge(*edge['nodes'], weight=edge['weight'])  \n",
    "    return graph\n",
    "\n",
    "def aggregate_list_nodes_and_edges(lis):\n",
    "    output = {'nodes': {}, 'edges': []}\n",
    "    for doc in lis:\n",
    "        for node in doc['nodes']:\n",
    "            if node in output['nodes']:\n",
    "                output['nodes'][node] += node['weight']\n",
    "            else:\n",
    "                output[node] = node\n",
    "        output['edges'].extend(doc['edges'])\n",
    "    return output\n",
    "\n",
    "def save_as_gexf(filename, graph):\n",
    "    nx.write_gexf(graph, os.path.normpath(filename))\n",
    "    \n",
    "def save_as_graphml(filename, graph):\n",
    "    nx.write_graphml(graph, os.path.normpath(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_filter(data, start, end):\n",
    "    \n",
    "    def datetime_interpretor(*args, default_tzinfo=tz.gettz('UTC'), **kwargs):\n",
    "        dt = parse(*args, **kwargs)[0]\n",
    "        return dt.replace(tzinfo=dt.tzinfo or default_tzinfo)\n",
    "    \n",
    "    start = parse(start, fuzzy_with_tokens=True)[0]\n",
    "    end = parse(end, fuzzy_with_tokens=True)[0]\n",
    "    \n",
    "    return [doc for doc in data if start <= doc['datetime'] < end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hists(input_, colour='r', n_bins=21, use_logx=True, **kwargs): # Now takes a title arguement to enable story titles to stay with stories\n",
    "    '''\n",
    "    Takes a list of 1D numpy arrays and Plots histograms.\n",
    "    \n",
    "    Args:\n",
    "        list_1D_arrays - a list of 1D numpy arrays.\n",
    "        \n",
    "    Returns:\n",
    "        histograms     - will output a figure of all 1D arrays as histograms.\n",
    "    '''\n",
    "    \n",
    "    style.use('seaborn')\n",
    "    \n",
    "    if n_bins > len(input_):\n",
    "        n_bins = len(input_) - 2\n",
    "    \n",
    "    if use_logx:\n",
    "        bins = logspace(log10(min(input_)), log10(max(input_)), n_bins)\n",
    "                                                                                                                             # arguement list.\n",
    "    else:\n",
    "        bins = linspace(min(input_), max(input_), n_bins)\n",
    "    \n",
    "    fig = figure()\n",
    "    \n",
    "    ax = gca()\n",
    "    \n",
    "    n, bins, patches = hist(input_, \n",
    "                             bins,\n",
    "                             #color=colour, \n",
    "                             #alpha=1,\n",
    "                             #density=True, \n",
    "                             ec='k',\n",
    "                             rwidth=0.9,\n",
    "                             #histtype='bar', \n",
    "                             #facecolor='blue',\n",
    "                             #log=True,\n",
    "                            #label=''\n",
    "                            )\n",
    "    \n",
    "    bin_centers = 0.5 * log10(bins[:-1] + bins[1:])\n",
    "    \n",
    "    col = bin_centers - min(bin_centers)\n",
    "    col /= max(col)\n",
    "    \n",
    "    cm = plt.cm.get_cmap('twilight_shifted')\n",
    "\n",
    "    for c, p in zip(col, patches):\n",
    "        plt.setp(p, 'facecolor', cm(c))\n",
    "\n",
    "#     fig.text(0.05, 0.5, 'Frequency', # Y-axis label\n",
    "#              horizontalalignment='center',\n",
    "#              verticalalignment='center', rotation=90)\n",
    "    \n",
    "#     fig.text(0.5, 0.01, 'Degree', # X-axis label\n",
    "#              horizontalalignment='center',\n",
    "#              verticalalignment='center')\n",
    "    \n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    xticks(fontsize=14)\n",
    "    yticks(fontsize=14)\n",
    "    legend(title=kwargs.get('title', ''), title_fontsize=16)\n",
    "    fname = kwargs.get('fname', None)\n",
    "    if fname is not None:\n",
    "        savefig(fname)\n",
    "    #fig.legend(loc=(0.16, 0.72), fontsize=9, frameon=False)\n",
    "    if kwargs.get('show', False):\n",
    "        show()\n",
    "        \n",
    "def assign_partition(graph, partition):\n",
    "    for node, cluster in partition.items():\n",
    "        graph.node[node]['Cluster'] = cluster\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    mag1 = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    mag2 = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (mag1 * mag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['sentence significance', \n",
    "          'document significance', \n",
    "          'sentence and document significance',\n",
    "          'sentence count',\n",
    "          'document count', \n",
    "          'sentence and document count'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build graphs and save as .graphml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 132975/132975 [00:00<00:00, 152474.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 824427/824427 [00:04<00:00, 181758.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 824427/824427 [00:04<00:00, 181289.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 132975/132975 [00:00<00:00, 165186.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 824427/824427 [00:04<00:00, 187374.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 824427/824427 [00:04<00:00, 171947.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for fname in fnames:\n",
    "    data = load_pickle(folder + r'\\graphs' + fname + ' list.pkl')\n",
    "    graph = build_graph(aggregate_list_nodes_and_edges(data), use_node_weight=False)\n",
    "    del data\n",
    "    save_as_graphml(folder + r'\\graphml' + fname + r'.graphml', graph)\n",
    "    del graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell will run the 'effect of co-occurrence rule on degree' experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "for fname, label in zip(fnames, labels):\n",
    "    graph = ig.read(folder + r'\\graphml' + fname + r'.graphml')\n",
    "    plot_hists(\n",
    "    array(graph.strength(weights=graph.es['weight'])), \n",
    "    use_logx=True, \n",
    "    fname=r'demo\\figs' \\\n",
    "    + fname \\\n",
    "    + ' ' + ' '.join({'person', 'location', 'organization'} - omitted_tags)\n",
    "    + ' (weighted).png',\n",
    "    title=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below cell will calculate the nodes with high degree centrality for each co-occurrence rule and save as a excel spreadsheet in the Networkx degree folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n"
     ]
    }
   ],
   "source": [
    "t50_savenames = [folder + r'\\degree%s top 50.csv' % f for f in fnames]\n",
    "pkl_savenames = [folder + r'\\degree%s degree.pkl' % f for f in fnames]\n",
    "\n",
    "for fname, t50, pkl in zip(fnames, t50_savenames, pkl_savenames):\n",
    "    graph = ig.read(folder + r'\\graphml' + fname + r'.graphml')\n",
    "    unweighted_degree = array(graph.degree())\n",
    "    idx = argsort(-unweighted_degree)\n",
    "    unweighted_degree = unweighted_degree[idx]\n",
    "    unweighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "    \n",
    "    weighted_degree = array(graph.strength(weights=graph.es['weight']))\n",
    "    idx = argsort(-weighted_degree)\n",
    "    weighted_degree = weighted_degree[idx]\n",
    "    weighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "    \n",
    "    df = pd.DataFrame([unweighted_ents[: 50], unweighted_degree[: 50], weighted_ents[: 50], weighted_degree[: 50]])\n",
    "    df.T.to_csv(t50)\n",
    "    save_pickle(pkl, pd.DataFrame([unweighted_ents, unweighted_degree, weighted_ents, weighted_degree]))\n",
    "    del graph\n",
    "    del idx\n",
    "    del weighted_degree\n",
    "    del unweighted_degree\n",
    "    del weighted_ents\n",
    "    del unweighted_ents\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below cell will calculate the nodes with high pagerank centrality for each co-occurrence rule and save as a excel spreadsheet in the Networkx page rank folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n"
     ]
    }
   ],
   "source": [
    "t50_savenames = [folder + r'\\page rank%s top 50.csv' % f for f in fnames]\n",
    "pkl_savenames = [folder + r'\\page rank%s page rank.pkl' % f for f in fnames]\n",
    "\n",
    "for fname, t50, pkl in zip(fnames, t50_savenames, pkl_savenames):\n",
    "    graph = ig.read(folder + r'\\graphml' + fname + r'.graphml')\n",
    "    unweighted_pagerank = array(graph.pagerank())\n",
    "    idx = argsort(-unweighted_pagerank)\n",
    "    unweighted_pagerank = unweighted_pagerank[idx]\n",
    "    unweighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "    \n",
    "    weighted_pagerank = array(graph.pagerank(weights=graph.es['weight']))\n",
    "    idx = argsort(-weighted_pagerank)\n",
    "    weighted_pagerank = weighted_pagerank[idx]\n",
    "    weighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "    \n",
    "    df = pd.DataFrame([unweighted_ents[: 50], unweighted_pagerank[: 50], weighted_ents[: 50], weighted_pagerank[: 50]])\n",
    "    df.T.to_csv(t50)\n",
    "    save_pickle(pkl, pd.DataFrame([unweighted_ents, unweighted_pagerank, weighted_ents, weighted_pagerank]))\n",
    "    del graph\n",
    "    del idx\n",
    "    del weighted_pagerank\n",
    "    del unweighted_pagerank\n",
    "    del weighted_ents\n",
    "    del unweighted_ents\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n",
      "Pickling data...\n"
     ]
    }
   ],
   "source": [
    "t50_savenames = [folder + r'\\eigen%s top 50.csv' % f for f in fnames]\n",
    "pkl_savenames = [folder + r'\\eigen%s eigen.pkl' % f for f in fnames]\n",
    "\n",
    "for fname, t50, pkl in zip(fnames, t50_savenames, pkl_savenames):\n",
    "    \n",
    "    graph = ig.read(folder + r'\\graphml' + fname + r'.graphml')\n",
    "    unweighted_eigen = array(graph.evcent())\n",
    "    idx = argsort(-unweighted_eigen)\n",
    "    unweighted_eigen = unweighted_eigen[idx]\n",
    "    unweighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "    \n",
    "    weighted_eigen = array(graph.evcent(weights=graph.es['weight']))\n",
    "    idx = argsort(-weighted_eigen)\n",
    "    weighted_eigen = weighted_eigen[idx]\n",
    "    weighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "    \n",
    "    df = pd.DataFrame([unweighted_ents[: 50], unweighted_eigen[: 50], weighted_ents[: 50], weighted_eigen[: 50]])\n",
    "    df.T.to_csv(t50)\n",
    "    save_pickle(pkl, pd.DataFrame([unweighted_ents, unweighted_eigen, weighted_ents, weighted_eigen]))\n",
    "    del graph\n",
    "    del idx\n",
    "    del weighted_eigen\n",
    "    del unweighted_eigen\n",
    "    del weighted_ents\n",
    "    del unweighted_ents\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [\n",
    "          r'\\01 sent sig',\n",
    "          r'\\03 doc sent sig',\n",
    "          r'\\04 sent count',\n",
    "          r'\\06 doc sent count',\n",
    "         ]\n",
    "\n",
    "folder = r'demo\\output\\person'\n",
    "\n",
    "labels = ['sentence significance',\n",
    "          'sentence and document significance',\n",
    "          'sentence count',\n",
    "          'sentence and document count'\n",
    "          ]\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n",
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fig, axs = subplots(4, 2, figsize=(9, 16), sharey=True)\n",
    "\n",
    "for i, fname, lab in zip(range(4), fnames, labels):\n",
    "    \n",
    "    style.use('seaborn')\n",
    "    graph = ig.read(folder + r'\\graphml' + fname + r'.graphml')\n",
    "\n",
    "    degrees = array(graph.strength())\n",
    "    eigen = array(graph.evcent())\n",
    "    axs[i, 0].scatter(degrees, eigen, s=3)\n",
    "    axs[i, 0].legend(title=lab, title_fontsize=11)\n",
    "    \n",
    "    weighted_degrees = array(graph.strength(weights=graph.es['weight']))\n",
    "    weighted_eigen = array(graph.evcent(weights=graph.es['weight']))\n",
    "    #page_ranks = array(graph.pagerank())\n",
    "    axs[i, 1].scatter(weighted_degrees, weighted_eigen, s=3)\n",
    "    axs[i, 1].legend(title=lab + ' (weighted)', title_fontsize=12)\n",
    "    \n",
    "    #data.append([[degrees, eigen], [weighted_degrees, weighted_eigen]])\n",
    "\n",
    "subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "ylabel('eigenvector centrality', fontsize=14)\n",
    "xlabel('degree', fontsize=14)\n",
    "gca().yaxis.set_label_coords(-1.23, 2.2)\n",
    "gca().xaxis.set_label_coords(-0.03, -0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = subplots(4, 2, figsize=(9, 16), sharey=True)\n",
    "\n",
    "for i, fname, lab in zip(range(4), fnames, labels):\n",
    "    \n",
    "    style.use('seaborn')\n",
    "    graph = ig.read(folder + r'\\graphml' + fname + r'.graphml')\n",
    "    \n",
    "    weighted_degrees = array(graph.strength(weights=graph.es['weight']))\n",
    "    weighted_eigen = array(graph.evcent(weights=graph.es['weight']))\n",
    "    #page_ranks = array(graph.pagerank())\n",
    "    axs[i, 0].scatter(data[i][0][0], data[i][0][1], s=3)\n",
    "    axs[i, 0].legend(title=lab, title_fontsize=12)\n",
    "\n",
    "    degrees = array(graph.strength())\n",
    "    eigen = array(graph.evcent())\n",
    "    axs[i, 1].scatter(data[i][1][0], data[i][1][1], s=3)\n",
    "    axs[i, 1].legend(title=lab + ' (weighted)', title_fontsize=11)\n",
    "    \n",
    "    #data.append([[weighted_degrees, weighted_eigen], [degrees, eigen]])\n",
    "\n",
    "subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "ylabel('eigenvector centrality', fontsize=14)\n",
    "xlabel('degree', fontsize=14)\n",
    "gca().yaxis.set_label_coords(-1.23, 2.2)\n",
    "gca().xaxis.set_label_coords(-0.03, -0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [\n",
    "         r'\\01 sent sig list per org',\n",
    "         r'\\02 doc sig list per org',\n",
    "         r'\\03 doc sent sig list per org',\n",
    "         r'\\04 sent count list per org',\n",
    "         r'\\05 doc count list per org',\n",
    "         r'\\06 doc sent count list per org',\n",
    "         ]\n",
    "\n",
    "\n",
    "labels = ['sentence significance', \n",
    "          'document significance', \n",
    "          'sentence and document significance',\n",
    "          'sentence count',\n",
    "          'document count', \n",
    "          'sentence and document count'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in fnames:\n",
    "    data = load_pickle(r'data' + fname + '.pkl')\n",
    "    graph = build_graph(aggregate_list_nodes_and_edges(data), use_node_weight=False)\n",
    "    del data\n",
    "    save_as_graphml(r'graphml' + fname + r'.graphml', graph)\n",
    "    del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname, label in zip(fnames, labels):\n",
    "    graph = ig.read(r'graphml' + fname + r'.graphml')\n",
    "    graph.es.select(weight=0).delete()\n",
    "    plot_hists(\n",
    "    array(graph.degree()), \n",
    "    use_logx=True, \n",
    "    fname=r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Thesis__\\Figures\\Degree co ocurrence rule' \\\n",
    "    + fname \\\n",
    "    + '.png',\n",
    "    title=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ig.read(r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\11 Timeseries\\output\\person\\graphml\\01 sent sig.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ents = {'Donald Trump', 'Spot Development', 'United States', 'Ramadan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen = array(graph.evcent())\n",
    "ents = array([i['id'] for i in list(graph.vs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = {k: v for k, v in zip(ents, eigen)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle(\n",
    "r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\11 Timeseries\\output\\person\\graphs\\01 sent sig dic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code is for thresholding / removing trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(o):\n",
    "    with open(os.path.normpath(o['graph_data_path']), 'rb') as open_file:\n",
    "        o['input_data'] = pickle.load(open_file)\n",
    "        return o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_unique_entities(o):\n",
    "    print('Harvesting nodes...')\n",
    "    o['entity_set'] = set()\n",
    "    progress = tqdm(range(len(o['input_data'])))\n",
    "    for date in o['input_data']:\n",
    "        for node in o['input_data'][date]['nodes']:\n",
    "            o['entity_set'].add(node)\n",
    "        progress.update(1)\n",
    "    o['entity_set'] = list(o['entity_set'])\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_key(o):\n",
    "    o['ent_key'] = {ent: i for i, ent in enumerate(o['entity_set'])}\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeseries(o):\n",
    "    print('Building 2d array...')\n",
    "    o['arr'] = zeros((len(o['entity_set']), 365))\n",
    "    o['dates'] = sorted(list(o['input_data'].keys()))\n",
    "    for i in tqdm(range(365)):\n",
    "        for ent in o['input_data'][o['dates'][i]]['nodes']:\n",
    "            o['arr'][o['ent_key'][ent], i] = o['input_data'][o['dates'][i]]['nodes'][ent]\n",
    "    del o['input_data']\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(o):\n",
    "    print('Calculating absolute change over time...')\n",
    "    o['arr'] = abs(o['arr'][:, o['change_width']: ] - o['arr'][:, : -o['change_width']])\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(o):\n",
    "    print('Applying rolling functions...')\n",
    "    shape = o['arr'].shape[:-1] + (o['arr'].shape[-1] - o['window_size'] + 1, o['window_size'])\n",
    "    strides = o['arr'].strides + (o['arr'].strides[-1],)\n",
    "    o['rolling_window'] = np.lib.stride_tricks.as_strided(o['arr'], shape=shape, strides=strides)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_functions(o):\n",
    "    o['xbar'] = mean(o['rolling_window'], -1)\n",
    "    o['sigma'] = std(o['rolling_window'], -1)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(o):\n",
    "    print('Applying threshold...')\n",
    "    o['output'] = abs(o['arr'] - o['xbar'])\n",
    "    o['output'][o['output'] <= o['sigma'] * o['threshold']] = 0.0\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_dates(o):\n",
    "    o['dates'] = o['dates'][o['window_size'] - 1: ]\n",
    "    return o\n",
    "\n",
    "def offset_timeseries(o):\n",
    "    o['arr'] = o['arr'][:, o['window_size'] - 1:]\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_ent_key(o):\n",
    "    for ent in list(o['ent_key'].keys()):\n",
    "        o['ent_key'][o['ent_key'][ent]] = ent\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_peaking_entities(o):\n",
    "    temp = o['output'].copy()\n",
    "    o['peak_depth'] = o.get('peak_depth', 1)\n",
    "    o['peaking_entities'] = {date: [] for date in o['dates']}\n",
    "    print('Harvesting peaking entities...')\n",
    "    for depth in range(1, o['peak_depth'] + 1):\n",
    "        print('Peak depth: %d' % depth)\n",
    "        peaking_idx = argmax(temp, axis=0)\n",
    "        for j in tqdm(range(len(o['dates']))):\n",
    "            if temp[peaking_idx[j], j] > 0.0:\n",
    "                o['peaking_entities'][o['dates'][j]].append(o['ent_key'][peaking_idx[j]])     \n",
    "        temp[peaking_idx] = 0.0\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_peaking_entities(o):\n",
    "    temp = o['output'].copy()\n",
    "    o['peak_depth'] = o.get('peak_depth', 1)\n",
    "    o['peaking_entities'] = {date: [] for date in o['dates']}\n",
    "    print('Harvesting peaking entities...')\n",
    "    for depth in range(1, o['peak_depth'] + 1):\n",
    "        print('Peak depth: %d' % depth)\n",
    "        peaking_idx = argmax(temp, axis=0)\n",
    "        for j in tqdm(range(len(o['dates']))):\n",
    "            if temp[peaking_idx[j], j] > 0.0:\n",
    "                o['peaking_entities'][o['dates'][j]].append(o['ent_key'][peaking_idx[j]])     \n",
    "        temp[peaking_idx] = 0.0\n",
    "    return o\n",
    "\n",
    "def prune_documents(o):\n",
    "    with open(os.path.normpath(o['document_data_path']), 'rb') as open_file:\n",
    "        data = pickle.load(open_file)\n",
    "    progress = tqdm(range(len(data)))\n",
    "    print('Pruning documents...')\n",
    "    pruned_data = []\n",
    "    for doc in data:\n",
    "        date = doc['datetime']\n",
    "        if date in o['peaking_entities']:\n",
    "            for ent in o['peaking_entities'][date]:\n",
    "                if ent in doc['nodes']:\n",
    "                    pruned_data.append(doc)\n",
    "                    break\n",
    "        progress.update(1)\n",
    "    print('Saving...')\n",
    "    with open(os.path.normpath(o['where_to_save']), 'wb') as open_file:\n",
    "        pickle.dump(pruned_data, open_file)\n",
    "    print('Done.')\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply threshold to graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_data_path = r'output\\all\\graphs\\03 doc sent sig dic.pkl'\n",
    "document_data_path = r'output\\all\\graphs\\03 doc sent sig list.pkl'\n",
    "where_to_save = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\08 Network\\data\\wiki doc sent sig 1.9.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the effect of applying different thresholds\n",
    "\n",
    "mini_pipeline = compose(\n",
    "                        apply_threshold,\n",
    "                        offset_dates,\n",
    "                        offset_timeseries,\n",
    "                        map_functions,\n",
    "                        rolling_window,\n",
    "                        create_timeseries,\n",
    "                        create_entity_key,\n",
    "                        harvest_unique_entities,\n",
    "                        load_graph_data,\n",
    "                        )\n",
    "\n",
    "# Will apply threshold and prune documents\n",
    "\n",
    "full_pipeline = compose(\n",
    "                        prune_documents,\n",
    "                        harvest_peaking_entities,\n",
    "                        invert_ent_key,\n",
    "                        apply_threshold,\n",
    "                        offset_dates,\n",
    "                        offset_timeseries,\n",
    "                        map_functions,\n",
    "                        rolling_window,\n",
    "                        create_timeseries,\n",
    "                        create_entity_key,\n",
    "                        harvest_unique_entities,\n",
    "                        load_graph_data,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvesting nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 365/365 [00:00<00:00, 943.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 2d array...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 365/365 [00:01<00:00, 337.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying rolling functions...\n",
      "Applying threshold...\n",
      "Harvesting peaking entities...\n",
      "Peak depth: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 359/359 [00:00<00:00, 179598.66it/s]\n",
      "  0%|                                                                                        | 0/59575 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████▍                               | 33497/59575 [00:00<00:00, 332398.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 59575/59575 [00:22<00:00, 332398.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "o = full_pipeline(\n",
    "                  dict(\n",
    "                       graph_data_path=graph_data_path,\n",
    "                       document_data_path=document_data_path,\n",
    "                       where_to_save=where_to_save,\n",
    "                       window_size=7, # Size of the window we will consider\n",
    "                       threshold=1.9, # Standard deviations. x > 2 = < 5% of normal distribution\n",
    "                       peak_depth=1,\n",
    "                       ),\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entity_timeseries(o, entity, *args, **kwargs):\n",
    "    xformat = kwargs.get('xformat', '%b')\n",
    "    style.use(kwargs.get('style', 'seaborn'))\n",
    "    figure(figsize=(10, 6))\n",
    "    [plot(o['dates'], o[arg][o['ent_key'][entity]] / max(o[arg][o['ent_key'][entity]])) for arg in args]\n",
    "    ax = gca()\n",
    "    setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    ax.xaxis.set_major_formatter(DateFormatter(xformat))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    ax.set_xlabel(kwargs.get('xlabel', 'Date (2017)'), fontsize=19)\n",
    "    ax.set_ylabel(kwargs.get('ylabel', 'Weighted Degree'), fontsize=19)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_range(start, end):\n",
    "    start = dt.strptime(start, '%d %b %Y').date()\n",
    "    end = dt.strptime(end, '%d %b %Y').date()\n",
    "    step = timedelta(days=1)\n",
    "    output = []\n",
    "    while start < end:\n",
    "        output.append(start)\n",
    "        start += step\n",
    "    return sorted(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gtd_timeseries(data, key, val, start='01 Jan 2017', end='01 Jan 2018'):\n",
    "    dt_range = create_date_range(start, end)\n",
    "    dt_key = {date: i for i, date in enumerate(dt_range)}\n",
    "    output = zeros(len(dt_key))\n",
    "    for date in data:\n",
    "        for e in data[date]:\n",
    "            if e[key] == val:\n",
    "                output[dt_key[date]] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gtd_comparison_entity_timeseries(o, tag, gtd_ent, ent, *args, **kwargs):\n",
    "    \n",
    "    xformat = kwargs.get('xformat', '%b')\n",
    "    style.use(kwargs.get('style', 'default'))\n",
    "    figure(figsize=(10, 6))\n",
    "    plot(o['dates'], o['output'][o['ent_key'][ent]], c='k', label='weighted degree')\n",
    "    ax1 = gca()\n",
    "    setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax1.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    ax1.set_xlabel(kwargs.get('xlabel', 'Date (2017)'), fontsize=19)\n",
    "    ax1.set_ylabel(kwargs.get('y1label', 'Weighted degree'), fontsize=19)\n",
    "    ax1.yaxis.label.set_color('k')\n",
    "    \n",
    "    data = \\\n",
    "    load_pickle(r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\GTD\\CritIII all regions GTD 1.pkl')\n",
    "    \n",
    "    start = kwargs.get('start', '01 Jan 2017')\n",
    "    end = kwargs.get('end', '01 Jan 2018')\n",
    "\n",
    "    gtd_timeseries = create_gtd_timeseries(data, tag, gtd_ent, start, end)\n",
    "    dt_range = create_date_range(start, end)\n",
    "    dt_key = {date: i for i, date in enumerate(dt_range)}\n",
    "    \n",
    "    ax2 = gca().twinx()\n",
    "    ax2.plot(dt_range, gtd_timeseries, c='r', alpha=0.5)\n",
    "    ax2.set_ylabel(kwargs.get('y2label', 'No. of terror events'), fontsize=19)\n",
    "    ax2.yaxis.set_label_coords(1.05, 0.5)\n",
    "    ax2.tick_params(axis='y', which='major', labelsize=15)\n",
    "    ax2.locator_params(nbins=2)\n",
    "    ax2.yaxis.label.set_color('r')\n",
    "    ax2.xaxis.set_major_formatter(DateFormatter(xformat))\n",
    "    gcf().subplots_adjust(bottom=0.15)\n",
    "    show()\n",
    "\n",
    "#ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "#ax.tick_params(axis='both', which='minor', labelsize=15)\n",
    "# gca().set_xlabel('Date (2017)', fontsize=12)\n",
    "# gca().yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "#gca().locator_params(nbins=4)\n",
    "# show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\pandas\\plotting\\_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: 'set_params()' not defined for locator of type <class 'pandas.plotting._converter.PandasAutoDateLocator'>\n"
     ]
    }
   ],
   "source": [
    "gtd_comparison_entity_timeseries(o, 'city', 'London', 'London')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: 'set_params()' not defined for locator of type <class 'pandas.plotting._converter.PandasAutoDateLocator'>\n"
     ]
    }
   ],
   "source": [
    "gtd_comparison_entity_timeseries(o, 'city', 'Kabul', 'Kabul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1666 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1666/1666 [00:00<00:00, 89706.79it/s]"
     ]
    }
   ],
   "source": [
    "f = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Thesis__\\Figures\\Networks\\london bridge.graphml'\n",
    "\n",
    "save_as_graphml(\n",
    "    f,\n",
    "    build_graph(\n",
    "    aggregate_list_nodes_and_edges(\n",
    "        datetime_filter(\n",
    "            load_pickle(where_to_save),\n",
    "            '03 Jun 2017', '04 Jun 2017')), use_node_weight=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_key(partition):\n",
    "    key = {}\n",
    "\n",
    "    for ent, p in partition.items():\n",
    "        if p not in key:\n",
    "            key[p] = [ent]\n",
    "        else:\n",
    "            key[p].append(ent)\n",
    "    return key\n",
    "\n",
    "def remove_stop_ents(igraph, stop_ents):\n",
    "    \n",
    "    keys = [(v['id'], v.index) for v in igraph.vs]\n",
    "    keys = list(sorted(keys, key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    for ent, idx in keys:\n",
    "        if ent in stop_ents:\n",
    "            igraph.delete_vertices(idx)\n",
    "            \n",
    "    return igraph\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_partition(f,\n",
    "                    use_weights=False,\n",
    "                    stop_ents=None):\n",
    "\n",
    "    igraph = ig.read(f)\n",
    "    \n",
    "    igraph = remove_stop_ents(igraph, stop_ents)\n",
    "    \n",
    "    if use_weights:\n",
    "        weights = igraph.es['weight']\n",
    "    else:\n",
    "        weights = None\n",
    "        \n",
    "    temp = igraph.community_leading_eigenvector(weights=weights)\n",
    "\n",
    "    partition = {}\n",
    "    \n",
    "    for cluster, lis in enumerate(temp):\n",
    "        for idx in lis:\n",
    "            partition[igraph.vs['id'][idx]] = cluster\n",
    "\n",
    "    return igraph, partition\n",
    "    \n",
    "\n",
    "def get_tops(f, \n",
    "             n, \n",
    "             n_best,\n",
    "             stop_ents=None,\n",
    "             use_part_weights=False,\n",
    "             use_cent_weights=False,\n",
    "             ):\n",
    "\n",
    "    \n",
    "    igraph, partition = eigen_partition(f, use_weights=use_part_weights, stop_ents=stop_ents)\n",
    "    \n",
    "    counts = Counter(partition.values())\n",
    "    \n",
    "    sum_ = sum(list(counts.values()))\n",
    "    \n",
    "    counts = {k: v / sum_ * 100 for k, v in counts.items()}.items()\n",
    "    \n",
    "    key = create_key(partition)\n",
    "    \n",
    "    if use_cent_weights:\n",
    "        weights = igraph.es['weight']\n",
    "    else:\n",
    "        weights = None\n",
    "    \n",
    "    lookup = array(igraph.evcent(weights=weights))\n",
    "\n",
    "    lookup = {k['id']: e for k, e in zip(igraph.vs, igraph.evcent(weights=weights))}\n",
    "    \n",
    "    maxi = {}\n",
    "    \n",
    "    for _ in range(n):\n",
    "        for p in key:\n",
    "            if key[p]:\n",
    "                m = key[p][0]\n",
    "                idx = {ent['id']: i for ent, i in zip(igraph.vs, range(len(list(igraph.vs))))}\n",
    "                for ent in key[p][1:]:\n",
    "                    if ent in lookup:\n",
    "                        if lookup[ent]> lookup[m]:\n",
    "                            m = ent\n",
    "                if p in maxi:\n",
    "                    maxi[p].append(m)\n",
    "                else:\n",
    "                    maxi[p] = [m]\n",
    "                igraph.delete_vertices(idx[m])\n",
    "                key[p].remove(m)\n",
    "                \n",
    "    best = list(sorted(counts, key=lambda x: x[1], reverse=True))\n",
    "                \n",
    "    return [(i[1], maxi[i[0]]) for i in best[:n_best]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = get_tops(f, 10, 10, use_part_weights=True, use_cent_weights=True, stop_ents=stop_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.21359223300971, London Ambulance Service, Bridge, Mark Rowley, Bridge Tube, Borough Market, Metropolitan Police, British Transport Police, Westminster, Metropolitan Police Service, Sadiq Khan\n",
      "18.446601941747574, Emmanuel Macron, Us Central Intelligence Agency, Iran, Wall Street Journal, Ayatollah Ruhollah Khomeini, Ayatollah Khomeini, Michael D'andrea, Saudi Arabia, Fethullah Gulen, CIA\n",
      "17.475728155339805, ISIL, London, Al Qaida, Ariana Grande, Osama Bin Laden, Omar Mateen, Jose Aznar, George W Bush, Middle East, Paris\n",
      "16.50485436893204, Iraq, Mosul, Syria, Raqqa, Syrian Observatory For Human Rights, Us Backed, SDF, United Nations, Cihan Sheikh Ehmed, Aamaq\n",
      "12.62135922330097, Narendra Modi, North Atlantic Treaty Organization, Kabul, Jens Stoltenberg, Muhammad Ashraf Ghani, Jalaluddin Haqqani, Rahmatullah Nabil, Anas Haqqani, Ataollah Khogiani, Nangarhar\n",
      "5.825242718446602, Theresa May, West, Khalid Masood, Midlands, Conservative Party, Labour Party\n",
      "2.912621359223301, Manchester, England, Arena\n"
     ]
    }
   ],
   "source": [
    "for i in tops:\n",
    "    print(i[0], *(j for j in i[1]), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netx_get_tops(f, \n",
    "             n, \n",
    "             n_best,\n",
    "             start, end,\n",
    "             stop_ents=None,\n",
    "             use_cent_weights=False,\n",
    "             ):\n",
    "    \n",
    "    \n",
    "    netx = build_graph(\n",
    "            aggregate_list_nodes_and_edges(\n",
    "                datetime_filter(\n",
    "                    load_pickle(where_to_save),\n",
    "                    start, end)), use_node_weight=False)\n",
    "    \n",
    "    netx.remove_nodes_from(stop_ents)\n",
    "    \n",
    "    save_as_graphml(r'temp\\temp.graphml', netx)\n",
    "    \n",
    "    partition = community.best_partition(netx)\n",
    "    \n",
    "    del netx\n",
    "    \n",
    "    igraph = ig.read(r'temp\\temp.graphml')\n",
    "    \n",
    "    counts = Counter(partition.values())\n",
    "    \n",
    "    sum_ = sum(list(counts.values()))\n",
    "    \n",
    "    counts = {k: v / sum_ * 100 for k, v in counts.items()}.items()\n",
    "    \n",
    "    key = create_key(partition)\n",
    "    \n",
    "    if use_cent_weights:\n",
    "        weights = igraph.es['weight']\n",
    "    else:\n",
    "        weights = None\n",
    "    \n",
    "    lookup = array(igraph.strength(weights=weights))\n",
    "\n",
    "    lookup = {k['id']: e for k, e in zip(igraph.vs, lookup)}\n",
    "    \n",
    "    maxi = {}\n",
    "    \n",
    "    for _ in range(n):\n",
    "        for p in key:\n",
    "            if key[p]:\n",
    "                m = key[p][0]\n",
    "                idx = {ent['id']: i for ent, i in zip(igraph.vs, range(len(list(igraph.vs))))}\n",
    "                for ent in key[p][1:]:\n",
    "                    if ent in lookup:\n",
    "                        if lookup[ent]> lookup[m]:\n",
    "                            m = ent\n",
    "                if p in maxi:\n",
    "                    maxi[p].append(m)\n",
    "                else:\n",
    "                    maxi[p] = [m]\n",
    "                igraph.delete_vertices(idx[m])\n",
    "                key[p].remove(m)\n",
    "                \n",
    "    best = list(sorted(counts, key=lambda x: x[1], reverse=True))\n",
    "                \n",
    "    return [(i[1], maxi[i[0]]) for i in best[:n_best]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/1666 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1666/1666 [00:00<00:00, 111880.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.126213592233007, London Ambulance Service, Theresa May, Borough Market, Bridge, Westminster, Mark Rowley, Metropolitan Police, United Kingdom, Bridge Tube, British Transport Police\n",
      "23.300970873786408, ISIL, West, Al Qaida, Manchester, Jose Aznar, England, Omar Mateen, Osama Bin Laden, Paris, London\n",
      "20.388349514563107, Iran, Us Central Intelligence Agency, Kabul, Wall Street Journal, Ayatollah Ruhollah Khomeini, Saudi Arabia, Fethullah Gulen, Turkey, Muhammad Ashraf Ghani, CIA\n",
      "15.53398058252427, Raqqa, Syria, Syrian Observatory For Human Rights, Iraq, United Nations, SDF, Us Backed, Mosul, Cihan Sheikh Ehmed, Euphrates\n",
      "11.650485436893204, Narendra Modi, North Atlantic Treaty Organization, Jalaluddin Haqqani, Emmanuel Macron, Rahmatullah Nabil, Anas Haqqani, Jens Stoltenberg, Ataollah Khogiani, Nangarhar, Jalalabad\n"
     ]
    }
   ],
   "source": [
    "start, end = '03 Jun 2017', '04 Jun 2017'\n",
    "\n",
    "tops = netx_get_tops(where_to_save, 10, 10, start, end, stop_ents=stop_ents, use_cent_weights=True)\n",
    "for i in tops:\n",
    "    print(i[0], *(j for j in i[1]), sep=', ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_cluster(fname, start='31 dec 2016', end='01 Jan 2018', use_weights=False):\n",
    "    \n",
    "    netx = build_graph(\n",
    "        aggregate_list_nodes_and_edges(\n",
    "            datetime_filter(\n",
    "                load_pickle(fname),\n",
    "                start, end)), use_node_weight=False)\n",
    "    \n",
    "    save_as_graphml(r'temp\\temp.graphml', netx)\n",
    "    \n",
    "    del netx\n",
    "    \n",
    "    igraph = ig.read(r'temp\\temp.graphml')\n",
    "    \n",
    "    if use_weights:\n",
    "        weights = igraph.es['weight']\n",
    "    else:\n",
    "        weights = None\n",
    "        \n",
    "    partition = igraph.community_leading_eigenvector(weights=weights)\n",
    "    \n",
    "    key = {}\n",
    "    \n",
    "    for cluster, lis in enumerate(partition):\n",
    "        for ent in lis:\n",
    "            key[igraph.vs['id'][ent]] = cluster\n",
    "            \n",
    "    del igraph\n",
    "            \n",
    "    netx = nx.read_graphml(r'temp\\temp.graphml')\n",
    "\n",
    "    for ent, cluster in key.items():\n",
    "        netx.node[ent]['partition'] = cluster\n",
    "\n",
    "    return netx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "del o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtd_doc_count_comparison(data, \n",
    "                             use_rolling_mean=False, \n",
    "                             window_size=5, \n",
    "                             start='01 Jan 2017',\n",
    "                             end='01 Jan 2018',\n",
    "                             **kwargs):\n",
    "    \n",
    "    def rolling_mean(arr, window_size=window_size):\n",
    "        shape = arr.shape[:-1] + (arr.shape[-1] - window_size + 1, window_size)\n",
    "        strides = arr.strides + (arr.strides[-1],)\n",
    "        return mean(np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides), axis=1)\n",
    "    \n",
    "    xformat = kwargs.get('xformat', '%b')\n",
    "    style.use(kwargs.get('style', 'default'))\n",
    "    \n",
    "    figure(figsize=(10, 6))\n",
    "    \n",
    "    dt_range = create_date_range(start, end)\n",
    "    dt_key = {date: i for i, date in enumerate(dt_range)}\n",
    "    \n",
    "    data_timeseries = zeros(len(dt_range))\n",
    "    \n",
    "    for doc in data:\n",
    "        if doc['datetime'].date() in dt_key:\n",
    "            data_timeseries[dt_key[doc['datetime'].date()]] += 1\n",
    "            \n",
    "    if use_rolling_mean:\n",
    "        data_timeseries = rolling_mean(data_timeseries)\n",
    "        offset = window_size - 1\n",
    "    else:\n",
    "        offset = 0\n",
    "    \n",
    "    plot(dt_range[offset:], data_timeseries, c='k', label='No. of documents')\n",
    "    \n",
    "    ax1 = gca()\n",
    "    setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=15)\n",
    "    ax1.tick_params(axis='both', which='minor', labelsize=15)\n",
    "    ax1.set_xlabel(kwargs.get('xlabel', 'Date (2017)'), fontsize=19)\n",
    "    ax1.set_ylabel(kwargs.get('y1label', 'No. of documents'), fontsize=19)\n",
    "    ax1.yaxis.label.set_color('k')\n",
    "    \n",
    "    gtd_data = \\\n",
    "    load_pickle(r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\GTD\\CritIII all regions GTD 1.pkl')\n",
    "    \n",
    "    start = kwargs.get('start', '01 Jan 2017')\n",
    "    end = kwargs.get('end', '01 Jan 2018')\n",
    "\n",
    "    gtd_timeseries = zeros(len(dt_range))\n",
    "\n",
    "    for date in dt_range:\n",
    "        for e in gtd_data[date]:\n",
    "            gtd_timeseries[dt_key[date]] += 1\n",
    "            \n",
    "    if use_rolling_mean:\n",
    "        gtd_timeseries = rolling_mean(gtd_timeseries)\n",
    "\n",
    "    ax2 = gca().twinx()\n",
    "    ax2.plot(dt_range[offset:], gtd_timeseries, c='r')\n",
    "    ax2.set_ylabel(kwargs.get('y2label', 'No. of terror events'), fontsize=19)\n",
    "    ax2.yaxis.set_label_coords(1.05, 0.5)\n",
    "    ax2.tick_params(axis='y', which='major', labelsize=15)\n",
    "    ax2.locator_params(nbins=2)\n",
    "    ax2.yaxis.label.set_color('r')\n",
    "    ax2.xaxis.set_major_formatter(DateFormatter(xformat))\n",
    "    ax2.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "    gcf().subplots_adjust(bottom=0.15)\n",
    "    #legend()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n"
     ]
    }
   ],
   "source": [
    "data = load_pickle(r'data\\wiki.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: 'set_params()' not defined for locator of type <class 'pandas.plotting._converter.PandasAutoDateLocator'>\n"
     ]
    }
   ],
   "source": [
    "gtd_doc_count_comparison(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: 'set_params()' not defined for locator of type <class 'pandas.plotting._converter.PandasAutoDateLocator'>\n"
     ]
    }
   ],
   "source": [
    "gtd_doc_count_comparison(data, use_rolling_mean=True, window_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Loading pickled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: 'set_params()' not defined for locator of type <class 'pandas.plotting._converter.PandasAutoDateLocator'>\n"
     ]
    }
   ],
   "source": [
    "gtd_doc_count_comparison(load_pickle(where_to_save), use_rolling_mean=True, window_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n",
      "Loading pickled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: 'set_params()' not defined for locator of type <class 'pandas.plotting._converter.PandasAutoDateLocator'>\n"
     ]
    }
   ],
   "source": [
    "gtd_doc_count_comparison(load_pickle(where_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Simon\\\\OneDrive - University of Exeter\\\\__Project__\\\\11 Timeseries\\\\output\\\\person\\\\graphml\\\\01 sent sig.graphml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-51873596fa25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\11 Timeseries\\output\\person\\graphml\\01 sent sig.graphml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munweighted_eigen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevcent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0munweighted_eigen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0munweighted_eigen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munweighted_eigen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0munweighted_ents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\igraph\\__init__.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(filename, *args, **kwds)\u001b[0m\n\u001b[0;32m   4061\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mloaded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4062\u001b[0m     \"\"\"\n\u001b[1;32m-> 4063\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mGraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4064\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\NLP37\\lib\\site-packages\\igraph\\__init__.py\u001b[0m in \u001b[0;36mRead\u001b[1;34m(klass, f, format, *args, **kwds)\u001b[0m\n\u001b[0;32m   2221\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no reader method for file format: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2222\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2224\u001b[0m     \u001b[0mLoad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Simon\\\\OneDrive - University of Exeter\\\\__Project__\\\\11 Timeseries\\\\output\\\\person\\\\graphml\\\\01 sent sig.graphml'"
     ]
    }
   ],
   "source": [
    "graph = ig.read(r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\11 Timeseries\\output\\person\\graphml\\01 sent sig.graphml')\n",
    "unweighted_eigen = array(graph.evcent())\n",
    "idx = argsort(-unweighted_eigen)\n",
    "unweighted_eigen = unweighted_eigen[idx]\n",
    "unweighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "\n",
    "weighted_eigen = array(graph.evcent(weights=graph.es['weight']))\n",
    "idx = argsort(-weighted_eigen)\n",
    "weighted_eigen = weighted_eigen[idx]\n",
    "weighted_ents = array([i['id'] for i in list(graph.vs)])[idx]\n",
    "\n",
    "df1 = pd.DataFrame([unweighted_ents[: 50], unweighted_eigen[: 50]])\n",
    "df3 = pd.DataFrame([unweighted_ents[: 50], unweighted_eigen[: 50], weighted_ents[: 50], weighted_eigen[: 50]])\n",
    "df1.T.to_csv(r'temp\\t501.csv')\n",
    "df3.T.to_csv(r'temp\\t503.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pickled data...\n"
     ]
    }
   ],
   "source": [
    "data = load_pickle(r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\07 Network\\Resolved 4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ents = {'Donald Trump', 'Ramadan', 'United States'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP37",
   "language": "python",
   "name": "nlp37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
