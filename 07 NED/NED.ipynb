{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below environment is required for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\n",
      "# packages in environment at C:\\Anaconda3\\envs\\NLP37:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_pytorch_select           1.1.0                       cpu  \n",
      "altair                    3.1.0                    py37_0    conda-forge\n",
      "asn1crypto                0.24.0                   py37_0  \n",
      "atomicwrites              1.3.0                    py37_1  \n",
      "attrs                     19.1.0                   py37_1  \n",
      "backcall                  0.1.0                    py37_0  \n",
      "beautifulsoup4            4.7.1                    pypi_0    pypi\n",
      "blas                      1.0                         mkl  \n",
      "boto                      2.49.0                   py37_0    anaconda\n",
      "boto3                     1.9.162                    py_0    anaconda\n",
      "botocore                  1.12.163                   py_0    anaconda\n",
      "branca                    0.3.1                      py_0    conda-forge\n",
      "bs4                       0.0.1                    pypi_0    pypi\n",
      "bz2file                   0.98                     py37_1    anaconda\n",
      "ca-certificates           2019.6.16            hecc5488_0    conda-forge\n",
      "cartopy                   0.17.0          py37hfff79a3_1006    conda-forge\n",
      "certifi                   2019.6.16                py37_1    conda-forge\n",
      "cffi                      1.12.3           py37h7a1dbc1_0  \n",
      "chardet                   3.0.4                    pypi_0    pypi\n",
      "colorama                  0.4.1                    py37_0  \n",
      "corenlp-xml-reader        0.1.3                    pypi_0    pypi\n",
      "corenlpy                  0.0.6                    pypi_0    pypi\n",
      "country-list              0.1.3                    pypi_0    pypi\n",
      "cryptography              2.7              py37h7a1dbc1_0  \n",
      "cudatoolkit               10.0.130                      0  \n",
      "cycler                    0.10.0                   py37_0  \n",
      "cymem                     1.31.2           py37he980bc4_0  \n",
      "cytoolz                   0.9.0.1          py37hfa6e2cd_1  \n",
      "decorator                 4.4.0                    py37_1  \n",
      "dill                      0.2.9                    py37_0  \n",
      "docutils                  0.14                     py37_0    anaconda\n",
      "en-core-web-lg            2.0.0                    pypi_0    pypi\n",
      "en-core-web-md            2.0.0                    pypi_0    pypi\n",
      "en-core-web-sm            2.0.0                    pypi_0    pypi\n",
      "entrypoints               0.3                   py37_1000    conda-forge\n",
      "folium                    0.9.1                      py_0    conda-forge\n",
      "freetype                  2.9.1                ha9979f8_1  \n",
      "funcy                     1.12                     pypi_0    pypi\n",
      "future                    0.17.1                   py37_0  \n",
      "fuzzywuzzy                0.17.0                   pypi_0    pypi\n",
      "gensim                    3.4.0            py37hfa6e2cd_0    anaconda\n",
      "geographiclib             1.49                     pypi_0    pypi\n",
      "geopy                     1.20.0                   pypi_0    pypi\n",
      "geos                      3.7.2                habb2df7_1    conda-forge\n",
      "icc_rt                    2019.0.0             h0cc432a_1  \n",
      "icu                       58.2                 ha66f8fd_1  \n",
      "idna                      2.8                      pypi_0    pypi\n",
      "imbalanced-learn          0.5.0                    pypi_0    pypi\n",
      "importlib_metadata        0.17                     py37_1  \n",
      "intel-openmp              2019.4                      245  \n",
      "ipykernel                 5.1.1            py37h39e3cac_0  \n",
      "ipython                   7.6.0            py37h39e3cac_0  \n",
      "ipython_genutils          0.2.0                    py37_0  \n",
      "jedi                      0.13.3                   py37_0  \n",
      "jinja2                    2.10.1                   py37_0  \n",
      "jmespath                  0.9.4                      py_0    anaconda\n",
      "joblib                    0.13.2                   py37_0  \n",
      "jpeg                      9b                   hb83a4c4_2  \n",
      "jsonschema                3.0.1                    py37_0    conda-forge\n",
      "jupyter_client            5.2.4                    py37_0  \n",
      "jupyter_core              4.4.0                    py37_0  \n",
      "kiwisolver                1.1.0            py37ha925a31_0  \n",
      "libpng                    1.6.37               h2a8f88b_0  \n",
      "libsodium                 1.0.16               h9d3ae62_0  \n",
      "libtiff                   4.0.10               hb898794_2  \n",
      "m2w64-gcc-libgfortran     5.3.0                         6  \n",
      "m2w64-gcc-libs            5.3.0                         7  \n",
      "m2w64-gcc-libs-core       5.3.0                         7  \n",
      "m2w64-gmp                 6.1.0                         2  \n",
      "m2w64-libwinpthread-git   5.0.0.4634.697f757               2  \n",
      "markupsafe                1.1.1            py37he774522_0  \n",
      "matplotlib                3.1.0            py37hc8f65d3_0  \n",
      "mkl                       2019.4                      245  \n",
      "mkl-service               2.0.2            py37he774522_0  \n",
      "mkl_fft                   1.0.12           py37h14836fe_0  \n",
      "mkl_random                1.0.2            py37h343c172_0  \n",
      "more-itertools            7.0.0                    py37_0  \n",
      "msgpack-numpy             0.4.4.2                  py37_0  \n",
      "msgpack-python            0.5.6            py37he980bc4_1  \n",
      "msys2-conda-epoch         20160418                      1  \n",
      "murmurhash                0.28.0           py37he025d50_0  \n",
      "networkx                  2.3                        py_0  \n",
      "ninja                     1.9.0            py37h74a9793_0  \n",
      "nltk                      3.4.1                    py37_0  \n",
      "numexpr                   2.6.9            py37hdce8814_0  \n",
      "numpy                     1.16.4           py37h19fb1c0_0  \n",
      "numpy-base                1.16.4           py37hc3f5095_0  \n",
      "olefile                   0.46                     py37_0  \n",
      "openssl                   1.1.1c               hfa6e2cd_0    conda-forge\n",
      "owslib                    0.18.0                     py_0    conda-forge\n",
      "packaging                 19.0                     py37_0  \n",
      "pandas                    0.24.2           py37ha925a31_0  \n",
      "parso                     0.4.0                      py_0  \n",
      "pickleshare               0.7.5                    py37_0  \n",
      "pillow                    6.0.0            py37hdc69c19_0  \n",
      "pip                       19.1.1                   py37_0  \n",
      "plac                      0.9.6                    py37_0  \n",
      "pluggy                    0.12.0                     py_0  \n",
      "preshed                   1.0.1            py37he025d50_0  \n",
      "proj4                     5.2.0             h6538335_1004    conda-forge\n",
      "prompt_toolkit            2.0.9                    py37_0  \n",
      "protobuf                  3.8.0                    pypi_0    pypi\n",
      "py                        1.8.0                    py37_0  \n",
      "pycorenlp                 0.3.0                    pypi_0    pypi\n",
      "pycountry                 19.7.15                  pypi_0    pypi\n",
      "pycparser                 2.19                     py37_0  \n",
      "pyepsg                    0.4.0                      py_0    conda-forge\n",
      "pygments                  2.4.2                      py_0  \n",
      "pykdtree                  1.3.1           py37h452e1ab_1002    conda-forge\n",
      "pyldavis                  2.1.2                    pypi_0    pypi\n",
      "pyopenssl                 19.0.0                   py37_0  \n",
      "pyparsing                 2.4.0                      py_0  \n",
      "pyproj                    1.9.6           py37hfa6e2cd_1002    conda-forge\n",
      "pyqt                      5.9.2            py37h6538335_2  \n",
      "pyreadline                2.1                      py37_1  \n",
      "pyrsistent                0.15.3           py37hfa6e2cd_0    conda-forge\n",
      "pyshp                     2.1.0                      py_0    conda-forge\n",
      "pysocks                   1.7.0                    py37_0  \n",
      "pytest                    5.0.1                    py37_0  \n",
      "python                    3.7.1                h8c8aaf0_6  \n",
      "python-dateutil           2.8.0                    py37_0  \n",
      "python-igraph             0.7.1.post6              pypi_0    pypi\n",
      "python-levenshtein        0.12.0          py37hfa6e2cd_1001    conda-forge\n",
      "python-louvain            0.13                       py_0  \n",
      "pytorch                   1.1.0           py3.7_cuda100_cudnn7_1    pytorch\n",
      "pytz                      2019.1                     py_0  \n",
      "pyzmq                     18.0.0           py37ha925a31_0  \n",
      "qt                        5.9.7            vc14h73c81de_0  \n",
      "regex                     2018.07.11       py37hfa6e2cd_0  \n",
      "requests                  2.22.0                   py37_0  \n",
      "s3transfer                0.2.0                    py37_0    anaconda\n",
      "scikit-learn              0.21.2           py37h6288b17_0  \n",
      "scipy                     1.2.1            py37h29ff71c_0    anaconda\n",
      "selenium                  3.141.0          py37he774522_0  \n",
      "setuptools                41.0.1                   py37_0  \n",
      "shapely                   1.6.4           py37ha35856d_1006    conda-forge\n",
      "sip                       4.19.8           py37h6538335_0  \n",
      "six                       1.12.0                   py37_0  \n",
      "smart_open                1.8.4                      py_0    anaconda\n",
      "sner                      0.2.12                   pypi_0    pypi\n",
      "soupsieve                 1.9.2                    pypi_0    pypi\n",
      "spacy                     2.0.12           py37h8300f20_0  \n",
      "sqlite                    3.28.0               he774522_0  \n",
      "stanfordnlp               0.2.0                    pypi_0    pypi\n",
      "termcolor                 1.1.0                    py37_1  \n",
      "thinc                     6.10.3           py37h8300f20_0  \n",
      "tk                        8.6.8                hfa6e2cd_0  \n",
      "toolz                     0.9.0                    py37_0  \n",
      "torchvision               0.3.0              py37_cu100_1    pytorch\n",
      "tornado                   6.0.2            py37he774522_0  \n",
      "tqdm                      4.32.2                   pypi_0    pypi\n",
      "traitlets                 4.3.2                    py37_0  \n",
      "ujson                     1.35             py37hfa6e2cd_0  \n",
      "urllib3                   1.25.3                   pypi_0    pypi\n",
      "vc                        14.1                 h0510ff6_4  \n",
      "vincent                   0.4.4                      py_1    conda-forge\n",
      "vs2015_runtime            14.15.26706          h3a45250_4  \n",
      "wcwidth                   0.1.7                    py37_0  \n",
      "wheel                     0.33.4                   py37_0  \n",
      "win_inet_pton             1.1.0                    py37_0  \n",
      "wincertstore              0.2                      py37_0  \n",
      "word2vec                  0.9.4            py37h452e1ab_0  \n",
      "wordcloud                 1.5.0           py37hfa6e2cd_1000    conda-forge\n",
      "wrapt                     1.10.11          py37hfa6e2cd_2  \n",
      "xlrd                      1.2.0                    py37_0  \n",
      "xz                        5.2.4                h2fa13f4_4  \n",
      "zeromq                    4.3.1                h33f27b4_3  \n",
      "zipp                      0.5.1                      py_0  \n",
      "zlib                      1.2.11               h62dcd97_3  \n",
      "zstd                      1.3.7                h508b16e_0  \n"
     ]
    }
   ],
   "source": [
    "print(__import__('sys').version)\n",
    "!conda list -n NLP37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from toolz import compose, curry, concat\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from unicodedata import normalize\n",
    "\n",
    "import os, sys, re, datetime, pycountry\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    print('Loading file...')\n",
    "    with open(os.path.normpath(filename), 'rb') as open_file:\n",
    "        return pickle.load(open_file)\n",
    "\n",
    "def save_pickle(filename, data):\n",
    "    print('saving...')\n",
    "    with open(os.path.normpath(filename), 'wb') as open_file:\n",
    "        pickle.dump(data, open_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file...\n",
      "Loading file...\n"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\__Data__\\temp'\n",
    "\n",
    "KNOWN_PERSONS = r'lookups\\normalized_person_lookup.pkl'\n",
    "\n",
    "KNOWN_ORGANIZATIONS = r'lookups\\normalized_organization_lookup.pkl'\n",
    "\n",
    "KNOWN_LOCATIONS = r'lookups\\clean_location_lookup.pkl'\n",
    "\n",
    "RESOLVED_ENTITIES = r'lookups\\normalized_resolution_lookup.pkl'\n",
    "\n",
    "NORMALIZATION_PROTOCOL = 'NFKD' # Alt: NFKD, NFKC, NFC, NFD\n",
    "\n",
    "CTRL_REGEX = re.compile(r'[\\x00-\\x1f\\x7f-\\x9f]|\\s{2,}') # matches non-utf-8 encoded bytes\n",
    "\n",
    "# STOPWORDS = {'ENDOFDOC', 'Telegraph', 'Agence France-Presse', 'Guardian', 'God', 'Allah', 'Open',\n",
    "#              'Northen', 'North'}\n",
    "\n",
    "STOPTAGS = {'IDEOLOGY', 'HANDLE', 'CAUSE_OF_DEATH', 'TITLE', 'CRIMINAL_CHARGE', \n",
    "            'URL', 'EMAIL', 'RELIGION', 'MISC'}\n",
    "\n",
    "TRANSLATOR = {'PERSON': 'person',\n",
    "              'LOCATION': 'location',\n",
    "              'CITY': 'location',\n",
    "              'COUNTRY': 'location',\n",
    "              'STATE_OR_PROVINCE': 'location',\n",
    "              'NATIONALITY': 'location',\n",
    "              'ORGANIZATION': 'organization',\n",
    "              \n",
    "              }\n",
    "\n",
    "MANUAL_TRANS = load_pickle(r'lookups\\manual_trans.pkl')\n",
    "\n",
    "STOPENTS = {'5ft 5ins', 'Olaf', \n",
    "            'Beyonce-obsessed Tracey', 'A. Got', 'Spotify', \"'' Hidalgo\",\n",
    "            'PSGCredit', 'BST The Labour', 'ENDOFDOC', 'Central', 'Republic', 'BBC Monitoring', 'A. Headlines B. Main',\n",
    "            'B. Home', 'CITY', 'fold', 'C. Adverts', 'N.', 'CNN', 'Telegraph', 'Agence France-Presse', 'Guardian', 'God',\n",
    "            'Allah', 'Open', 'Northen', 'North', 'New York Times', 'Times', 'His Highness', 'Region', 'AFP',\n",
    "            'BBC Monitoring in Arabic', 'BBC',' 202:57', '229:19', '3in', 'Aa', 'U S', 'Interfax', 'Interfax Ukraine',\n",
    "            'A A', 'Aaaaa', 'PST', 'GMT', 'UTC', 'AM', 'PM', 'Santa Claus', 'Santa', 'URL', 'URLs', 'Express Tribune'\n",
    "            'Rotten Tomatoes', 'RSVP', 'Resident Evil', 'Daily Nation', 'Daily Star', 'Telegram'}\n",
    "\n",
    "\n",
    "PUBLICATIONS = {\n",
    "                'BBC', 'new york times', 'the guardian', 'telegraph', 'agence france presse', 'CNN', 'UPI', 'ukrinform',\n",
    "                'associated press', 'Aaj Shahzeb Khanzada', 'News', 'TV', 'Channel', 'Radio', 'Rapporteur'\n",
    "                'television', 'Online', 'Press',\n",
    "                }\n",
    "\n",
    "REPLACE_WITH_SPACE = {\n",
    "                      ' -RSB- ', ' -RRB- ', ' -LSB- ', '-', '  ', ' aka ', ' \" ', ' ; ', \n",
    "                      }\n",
    "\n",
    "ENTITY_MERGE_MAP = {\n",
    "                    ', D.C.': {'Washington': 'Washington DC'},\n",
    "                    'Petersburg': {'St.': 'St. Petersburg'}\n",
    "                    }\n",
    "\n",
    "REPLACE_WITH_EMPTY = {'/Telegraph', 'Tory Brexiter ', '/PA', '/FilmMagic Right', '-born', '``', '-owned', '-backed',\n",
    "                      'His Royal Majesty ', '-held', '.', 'Associated Press of ', '`', 'His Highness', 'Jankovska'}\n",
    "\n",
    "REPLACE_SPECIAL = load_pickle(r'lookups\\replace_special.pkl')\n",
    "\n",
    "\n",
    "REPLACE_TRANS = { **{i: ' ' for i in REPLACE_WITH_SPACE}, \n",
    "                  **{i: '' for i in REPLACE_WITH_EMPTY},\n",
    "                  **REPLACE_SPECIAL\n",
    "                 }\n",
    "\n",
    "REPLACE_TRANS = dict((re.escape(k), v) for k, v in REPLACE_TRANS.items())\n",
    "REPLACE_REGEX = re.compile('|'.join(REPLACE_TRANS.keys()))\n",
    "\n",
    "ACRO_REGEX = re.compile(r'^[A-Z]{2,3}$') # For finding abbreviations\n",
    "\n",
    "ACRO_REPLACE_TRANS = {i: ' ' for i in {' and ', ' of ', ' the '}}\n",
    "\n",
    "ACRO_REPLACE_TRANS = dict((re.escape(k), v) for k, v in ACRO_REPLACE_TRANS.items())\n",
    "ACRO_REPLACE_REGEX = re.compile('|'.join(ACRO_REPLACE_TRANS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_explorer(extension, directory):\n",
    "    '''\n",
    "    A generator to find filenames with a given extension within a given \n",
    "    directory\n",
    "    '''\n",
    "    ext_upper, ext_lower = extension.lower(), extension.upper()\n",
    "    for filename in os.listdir(os.path.normpath(directory)):\n",
    "        if filename.endswith(ext_upper) \\\n",
    "        or filename.endswith(ext_lower):\n",
    "            yield '%s/%s' % (directory, filename)\n",
    "            \n",
    "def replace_bad_chars(text, regex=CTRL_REGEX):\n",
    "    '''\n",
    "    Removes all ctrl characters such as '\\n'\n",
    "    and consecutive spaces\n",
    "    '''\n",
    "    return regex.sub(r' ', text)\n",
    "\n",
    "def normalize_unicode(text, protocol=NORMALIZATION_PROTOCOL):\n",
    "    '''\n",
    "    Map all characters to ascii encoding and then back to utf-8 in order\n",
    "    to remove invalid characters for nlp.\n",
    "    '''\n",
    "    return normalize(protocol, text).encode('ascii', 'ignore').decode('utf8')\n",
    "\n",
    "clean_text = compose(lambda x: x.replace('&', 'and'),\n",
    "                     replace_bad_chars,\n",
    "                     normalize_unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many functions use a variable called data as input. This variable is a list of dictionaries where a \n",
    "dictionary (doc) represents a document. \n",
    "\n",
    "doc = {\n",
    "\n",
    "        'id': the unique id of the doc,\n",
    "\n",
    "       'datetime': a datetime object of when the document was published,\n",
    "       \n",
    "       'sentences': a list of lists where each nested list represents a sentence\n",
    "       \n",
    "       }\n",
    "       \n",
    "      \n",
    "doc['sentences'] = [\n",
    "\n",
    "                   [ent1, ent2, ent3, ent4, ent1, ent2],\n",
    "                    \n",
    "                   [ent1, ent5, ent6, ent5, ent2],\n",
    "                    \n",
    "                   ]\n",
    "               \n",
    "               \n",
    "ent = {\n",
    "\n",
    "       'word': word that occured in the document,\n",
    "       \n",
    "       'words': the word split by a space i.e. ent['word'].split(' '),\n",
    "       \n",
    "       'tag': the type of entity i.e. person, location, organization,\n",
    "       \n",
    "        'len': number of characters in the word,\n",
    "    \n",
    "        'lemma': the ent word but lowercase i.e. ent['word'].lower(),\n",
    "       \n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_iter(data):\n",
    "    '''\n",
    "    Iterates through data structure one sentence at a time\n",
    "    '''\n",
    "    for doc in data:\n",
    "        for sentence in doc['sentences']:\n",
    "            yield sentence\n",
    "                \n",
    "def ent_iter(data, sentence_iter=sentence_iter):\n",
    "    '''\n",
    "    Iterates through data structure one ent at a time\n",
    "    '''\n",
    "    for sentence in sentence_iter(data):\n",
    "        for ent in sentence:\n",
    "            yield ent\n",
    "            \n",
    "def filter_empty(data):\n",
    "    '''\n",
    "    Removes empty sentences\n",
    "    '''\n",
    "    for doc in data:\n",
    "        for i, sentence in enumerate(doc['sentences']):\n",
    "            doc['sentences'][i] = list(filter(lambda x: x['word'].strip(), sentence))\n",
    "    return data\n",
    "\n",
    "def filter_size(data):\n",
    "    '''\n",
    "    Removes documents that are too short\n",
    "    '''\n",
    "    for doc in data:\n",
    "        for i, sentence in enumerate(doc['sentences']):\n",
    "            doc['sentences'][i] = list(filter(lambda x: len(x['word']) > 1, sentence))\n",
    "    return data\n",
    "\n",
    "def filter_publications(data, pubs=PUBLICATIONS):\n",
    "    '''\n",
    "    Removes entities which are the names of publications in the corpus\n",
    "    '''\n",
    "    for doc in data:\n",
    "        for i, sentence in enumerate(doc['sentences']):\n",
    "            new_sentence = []\n",
    "            for ent in sentence:\n",
    "                flag = True\n",
    "                for pub in pubs:\n",
    "                    if pub in ent['word'] or pub in ent['word'].lower():\n",
    "                        flag = False\n",
    "                        break\n",
    "                if flag:\n",
    "                    new_sentence.append(ent)\n",
    "            doc['sentences'][i] = new_sentence                 \n",
    "    return data\n",
    "\n",
    "def generate_entity(word, tag, resolved=None):\n",
    "    '''\n",
    "    Creates a new entity object\n",
    "    '''\n",
    "    ent = {'tag': tag, 'ner': tag}\n",
    "    ent['words'] = [x.strip() for x in word.split(' ') if x]\n",
    "    ent['n_words'] = len(ent['words'])\n",
    "    ent['word'] = ' '.join(ent['words']).strip()\n",
    "    ent['len'] = len(ent['word'])\n",
    "    ent['lemma'] = ent['word'].lower()\n",
    "    if resolved is not None:\n",
    "        ent['resolved'] = resolved\n",
    "    return ent\n",
    "\n",
    "def analyze_entities(\n",
    "                     data, \n",
    "                     ent_iter=ent_iter,\n",
    "                     generate_entity=generate_entity\n",
    "                     ):\n",
    "    '''\n",
    "    I decided to update the entity object from what was used in the previous notbook\n",
    "    so this function just changes all the entities to the new type\n",
    "    '''\n",
    "    print('Analyzing entitites...')\n",
    "    for sentence in tqdm(list(sentence_iter(data))):\n",
    "        for i, ent in enumerate(sentence):\n",
    "            sentence[i] = generate_entity(ent['word'], ent['tag'])\n",
    "    return data\n",
    "\n",
    "def resolution_check(\n",
    "                     data,\n",
    "                     sentence_iter=sentence_iter,\n",
    "                     load_pickle=load_pickle,\n",
    "                     resolved_entities=RESOLVED_ENTITIES,\n",
    "                     ):\n",
    "    '''\n",
    "    Checks to see if entities are resolved\n",
    "    '''\n",
    "    resolution_checker = load_pickle(resolved_entities)\n",
    "    print('Checking resolutions...')\n",
    "    for sentence in tqdm(list(sentence_iter(data))):\n",
    "        new_ents = []\n",
    "        for i, ent in enumerate(sentence):\n",
    "            \n",
    "            if 'resolved' in ent and ent['resolved'] == True:\n",
    "                continue\n",
    "                \n",
    "            if ent['word'] in resolution_checker:\n",
    "                ent['resolved'] = True\n",
    "            elif '-' in ent['word']:\n",
    "                words = [x for x in ent['word'].split('-') if x.strip()]\n",
    "                if all([word in resolution_checker for word in words]):\n",
    "                    sentence[i] = generate_entity(words[0], ent['tag'], True)\n",
    "                    #sentence[i]['resolved'] = True\n",
    "                    for word in words[1:]:\n",
    "                        word = generate_entity(word, ent['tag'], True)\n",
    "                        new_ents.append(word)\n",
    "                elif ent['word'].replace('-', ' ') in resolution_checker:\n",
    "                    sentence[i] = generate_entity(ent['word'].replace('-', ' '), ent['tag'], True)\n",
    "                else:\n",
    "                    ent['resolved'] = False\n",
    "            elif ent['len'] > 1 and ent['word'][-1] in {'s', 'S'} \\\n",
    "                 and ent['word'][: -1] in resolution_checker:\n",
    "                    sentence[i] = generate_entity(ent['word'][: -1], ent['tag'], True)\n",
    "            else:\n",
    "                ent['resolved'] = False\n",
    "                \n",
    "        sentence.extend(new_ents)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def manual_resolution(data, \n",
    "                      sentence_iter=sentence_iter,\n",
    "                      generate_entity=generate_entity,\n",
    "                      trans=MANUAL_TRANS):\n",
    "    print('Manually resolving entities...')\n",
    "    for sentence in tqdm(list(sentence_iter(data))):\n",
    "        for i, ent in enumerate(sentence):\n",
    "            if ent['word'] in trans:\n",
    "                ent = generate_entity(trans[ent['word']]['word'], trans[ent['word']]['tag'], True)\n",
    "                sentence[i] = ent\n",
    "    return data\n",
    "        \n",
    "def entity_type_set(data, tag, ent_iter=ent_iter):\n",
    "    '''\n",
    "    For viewing all unique entities of a type for debuggin perposes\n",
    "    Types: 'PERSON', 'COUNTRY', 'CITY', 'LOCATION', 'NATIONALITY', \n",
    "           'IDEOLOGY', 'HANDLE', 'CAUSE_OF_DEATH', 'TITLE',\n",
    "           'CRIMINAL_CHARGE', 'URL', 'EMAIL', 'RELIGION', 'MISC'\n",
    "    '''\n",
    "    output = set()\n",
    "    for ent in tqdm(list(ent_iter(data))):\n",
    "        if ent['tag'] == tag:\n",
    "            _append = [ent['word'], doc['id']]\n",
    "            if 'ner' in ent:\n",
    "                _append.append(ent['ner'])\n",
    "            output.add(tuple(_append))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leading_and_trailing_stops(data,\n",
    "                                      sentence_iter=sentence_iter,\n",
    "                                      generate_entity=generate_entity):\n",
    "    '''\n",
    "    Removes stopwords that occur at the start or end of the text of an entity\n",
    "    '''\n",
    "    \n",
    "    stops = {'the', 'of', 'and', 'for', 'this', 'what', 'most', 'much',\n",
    "             'over', 'qc', 'so', 'moreover', 'os', 'has', 'perhaps'}\n",
    "    changes = []\n",
    "    for sentence in tqdm(list(sentence_iter(data))):\n",
    "        for i, ent in enumerate(sentence):\n",
    "            if not ent['resolved']:\n",
    "                ent['words'] = [w for w in ent['words'] if w.strip()]\n",
    "                if len(ent['words']) > 1:\n",
    "                    if ent['words'][0].lower() in stops:\n",
    "                        ent['words'] = ent['words'][1:]\n",
    "                    if ent['words'][-1].lower() in stops:\n",
    "                        ent['words'] = ent['words'][:-1]\n",
    "                \n",
    "                sentence[i] = generate_entity(' '.join(ent['words']), ent['tag'], ent['resolved'])\n",
    "                if ent['word'] != sentence[i]['word']:\n",
    "                    changes.append((ent, sentence[i]))\n",
    "    return data\n",
    "\n",
    "def remove_press(data,\n",
    "                  sentence_iter=sentence_iter,\n",
    "                  generate_entity=generate_entity,\n",
    "                 pubs=PUBLICATIONS):\n",
    "    \n",
    "    pubs = {s.lower() for s in pubs}\n",
    "    for doc in data:\n",
    "        for i, sentence in enumerate(doc['sentences']):\n",
    "            new_sentence = []\n",
    "            for ent in sentence:\n",
    "                flag = False\n",
    "                for word in ent['words']:\n",
    "                    if word.lower() in pubs:\n",
    "                        flag = True\n",
    "                    if not flag:\n",
    "                        new_sentence.append(ent)\n",
    "            doc['sentences'][i] = new_sentence\n",
    "        doc['sentences'] = [sentence for sentence in doc['sentences'] if len(sentence) > 0]\n",
    "        \n",
    "    return data\n",
    "\n",
    "def resolve_s_endings(data,\n",
    "                      ent_iter=ent_iter,\n",
    "                      sentence_iter=sentence_iter,\n",
    "                      generate_entity=generate_entity):\n",
    "    \n",
    "    changes = []\n",
    "    ents = set([ent['word'] for ent in ent_iter(data)])\n",
    "    \n",
    "    for sentence in tqdm(list(sentence_iter(data))):\n",
    "        for i, ent1 in enumerate(sentence):\n",
    "            if not ent1['resolved']:\n",
    "                if ent1['len'] > 1 and ent1['word'].endswith('s'):\n",
    "                    for ent2 in ents:\n",
    "                        if ent1['word'][-1] == ent2:\n",
    "                            sentence[i] = generate_entity(ent2, ent1['tag'], ent['resolved'])\n",
    "                            changes.append((ent1, ent2))\n",
    "    save_pickle(r'logs\\resolve_s_endings.chge', changes)\n",
    "    return data\n",
    "    \n",
    "def resolve_multi_locations(data, \n",
    "                            sentence_iter=sentence_iter,\n",
    "                            generate_entity=generate_entity,\n",
    "                            locations=KNOWN_LOCATIONS,\n",
    "                            ):\n",
    "    locations = load_pickle(locations)\n",
    "    for sentence in sentence_iter(data):\n",
    "        new = []\n",
    "        for i, ent in enumerate(sentence):\n",
    "            if not ent['resolved']:\n",
    "                if ent['n_words'] > 1:\n",
    "                    if all([x.lower() in locations for x in ent['words']]):\n",
    "                        sentence[i] = generate_entity(locations[ent['words'][0].lower()], ent['tag'], ent['resolved'])\n",
    "                        for word in ent['words'][1:]:\n",
    "                            new.append(generate_entity(locations[word.lower()], ent['tag'], ent['resolved']))\n",
    "        for ent in new:\n",
    "            sentence.append(ent)\n",
    "    return data\n",
    "  \n",
    "    \n",
    "def resolve_double_orgs(data, \n",
    "                        sentence_iter=sentence_iter,\n",
    "                        generate_entity=generate_entity,\n",
    "                        organizations=KNOWN_ORGANIZATIONS,\n",
    "                        ):\n",
    "    changes = []\n",
    "    organizations = load_pickle(organizations)\n",
    "    for sentence in sentence_iter(data):\n",
    "        new = []\n",
    "        for i, ent in enumerate(sentence):\n",
    "            if not ent['resolved']:\n",
    "                if ent['n_words'] == 3:\n",
    "                    if ent['words'][1] in {'and', 'or'}:\n",
    "\n",
    "                        if ent['words'][0] in organizations and ent['words'][2] in organizations:\n",
    "                            sentence[i] = generate_entity(organizations[ent['words'][0]], ent['tag'], True)\n",
    "                            new_ent = generate_entity(organizations[ent['words'][2]], ent['tag'])\n",
    "                            new.append(new_ent, True)\n",
    "                            changes.append((sentence[i], new_ent))\n",
    "        for ent in new:\n",
    "            sentence.append(ent)\n",
    "    save_pickle(r'logs\\resolve_double_orgs.chge', changes)\n",
    "    return data\n",
    "\n",
    "\n",
    "def capitalize_entities(data,\n",
    "                        sentence_iter=sentence_iter):\n",
    "    '''\n",
    "    Capitalizes the first character of entities\n",
    "    '''\n",
    "    for sentence in sentence_iter(data):\n",
    "        for i, ent in enumerate(sentence):\n",
    "            #if not ent['resolved']:\n",
    "            if ent['n_words'] > 1 and ent['len'] > 3:\n",
    "                new = []\n",
    "                for j in range(ent['n_words']):\n",
    "                    w0 = ent['words'][j][0].upper()\n",
    "                    if len(ent['words'][j]) > 1:\n",
    "                        w1 = ent['words'][j][1:].lower()\n",
    "                        w = w0 + w1\n",
    "                    else:\n",
    "                        w = w0\n",
    "                    new.append(w)\n",
    "                sentence[i] = generate_entity(' '.join(new), ent['tag'], ent['resolved'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_data_structure(data):\n",
    "    '''\n",
    "    Aggregate and unpack NER output into a list\n",
    "    '''\n",
    "    output = []\n",
    "    print('Aggregating NER output into a list data structure...')\n",
    "    for batch in data:\n",
    "        for date in batch:\n",
    "            if date == 'n_articles':\n",
    "                continue\n",
    "            for id_ in batch[date]:\n",
    "                batch[date][id_]['datetime'] = date\n",
    "                if 'ents' in batch[date][id_]: # I decided to use 'sentences' as key\n",
    "                    batch[date][id_]['sentences'] = batch[date][id_]['ents']\n",
    "                    del batch[date][id_]['ents']\n",
    "                output.append(batch[date][id_])\n",
    "    return output\n",
    "\n",
    "def tag_correction(\n",
    "                   data,\n",
    "                   ent_iter=ent_iter, \n",
    "                   translator=TRANSLATOR\n",
    "                   ):\n",
    "    '''\n",
    "    Simply changes the name of the key used for the NER types\n",
    "    from 'ner' to 'tags' because I decided 'tag' was clearer \n",
    "    '''\n",
    "    print('Correcting tags...')\n",
    "    for ent in tqdm(list(ent_iter(data))):\n",
    "        if 'ner' not in ent:\n",
    "            ent['ner'] = ent['tag'].lower()\n",
    "            #del ent['ner']\n",
    "        if ent['tag'] in translator:\n",
    "            ent['tag'] = translator[ent['tag']]\n",
    "    return data\n",
    "    \n",
    "def stopword_filter(\n",
    "                    data,\n",
    "                    #stopwords=STOPWORDS, \n",
    "                    stoptags=STOPTAGS,\n",
    "                    stopents=STOPENTS,\n",
    "                    ):\n",
    "    '''\n",
    "    Filter out stopwords and unwanted enity types (stoptags)\n",
    "    '''\n",
    "    print('Filtering stopwords...')\n",
    "    for doc in tqdm(data):\n",
    "        filtered_sentences = []\n",
    "        for sentence in doc['sentences']:\n",
    "            sentence = [ent for ent in sentence \\\n",
    "                        if ent['tag'] not in stoptags \\\n",
    "                        #and ent['word'] not in stopwords \\\n",
    "                        and ent['word'] not in stopents]\n",
    "            if sentence: # check not empty\n",
    "                filtered_sentences.append(sentence)\n",
    "        doc['sentences'] = filtered_sentences\n",
    "    return data\n",
    "\n",
    "def entity_correction(\n",
    "                      data,\n",
    "                      sentence_iter=sentence_iter,\n",
    "                      trans=REPLACE_TRANS,\n",
    "                      regex=REPLACE_REGEX,\n",
    "                      merge_map=ENTITY_MERGE_MAP\n",
    "                      ):\n",
    "    '''\n",
    "    Implements some simple corrections that were noticed in the process\n",
    "    '''\n",
    "    table = {char: '' for char in '»|/\\\\\\{\\}[]!`\\\"\\\"£$%^&@?><!*(),.:;\\'~#=+_'}\n",
    "    table = str.maketrans(table)\n",
    "\n",
    "    def depunc(x, table=table):\n",
    "        return x.replace('\\'s', '').translate(table).strip()\n",
    "    \n",
    "    changes = []\n",
    "    print('Correcting entity errors...')\n",
    "    for sentence in tqdm(list(sentence_iter(data))):\n",
    "        deletes = []\n",
    "        new = []\n",
    "        for i, ent in enumerate(sentence):\n",
    "            dont_append_flag = False\n",
    "            start = ent['word']\n",
    "            \n",
    "            if len(ent['word']) > 3:\n",
    "                ent['word'] = depunc(ent['word'][0: 2]) + ent['word'][2: -2] + depunc(ent['word'][-2: ]).strip()\n",
    "            elif len(ent['word']) == 3:\n",
    "                ent['word'] = depunc(ent['word'][0]) + ent['word'][1] + depunc(ent['word'][2]).strip()\n",
    "            else:\n",
    "                ent['word'] = depunc(ent['word']).strip()\n",
    "            \n",
    "            if len(ent['word']) > 4:\n",
    "                try:\n",
    "                    int(ent['word'][:4])\n",
    "                    ent['word'] = ent['word'][4: ]\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if '-led' in ent['word']:\n",
    "                words = ent['word'].split('-led')\n",
    "                ent['word'] = words[0].strip()\n",
    "                for word in words[1: ]:\n",
    "                    stripped_word = word.strip()\n",
    "                    if stripped_word:\n",
    "                        new.append({'word': stripped_word, 'tag': ent['tag'], 'ner': ent['ner']})\n",
    "            # remove some errors\n",
    "            ent['word'] = regex.sub(lambda x: trans[re.escape(x.group(0))], ent['word']).strip()\n",
    "\n",
    "            # merge some obvious entities together that should have been joined\n",
    "            if ent['word'] in merge_map and sentence[i - 1]['word'] in merge_map[ent['word']]:\n",
    "                sentence[i - 1]['word'] = merge_map[ent['word']][sentence[i - 1]['word']]\n",
    "                deletes.append(i)\n",
    "                break\n",
    "\n",
    "            words = ent['word'].split(' ')\n",
    "            if ent['tag'] == 'person':\n",
    "                for j, word in enumerate(words):\n",
    "                    if word.isupper():# and len(word) > 1:\n",
    "                        words[j] = word[0].upper() + word[1: ].lower()\n",
    "                ent['word'] = ' '.join(words)\n",
    "            elif ent['tag'] == 'location':\n",
    "                if len(words) > 1 or len(ent['word']) > 3:\n",
    "                    for j, word in enumerate(words):\n",
    "                        if word.isupper() and len(word) > 2:\n",
    "                            words[j] = word[0].upper() + word[1: ].lower()\n",
    "                    ent['word'] = ' '.join(words)\n",
    "            elif ent['tag'] == 'organization':\n",
    "                for j, word in enumerate(words):\n",
    "                    if word.isupper() and len(word) > 3:\n",
    "                        words[j] = word[0].upper() + word[1: ].lower()\n",
    "                ent['word'] = ' '.join(words)\n",
    "            changes.append((start, ent['word']))\n",
    "\n",
    "        for i in sorted(deletes, reverse=True):\n",
    "            del sentence[i]   \n",
    "        for ent in new:\n",
    "            sentence.append(ent)\n",
    "    save_pickle(r'logs\\entity_correction.chge', changes)\n",
    "    return data\n",
    "\n",
    "def remove_titles(data, ent_iter=ent_iter):\n",
    "    '''\n",
    "    Removes Mr, Mrs, Ms etc...\n",
    "    '''\n",
    "    regex = re.compile(r'M(?:s|r|rs)\\.? ')# | [\\w\\d]\\. ')\n",
    "    print('Removing titles...')\n",
    "    #corrected = []\n",
    "    for ent in tqdm(list(ent_iter(data))):\n",
    "        if regex.search(ent['word']):# and not all([ent['word'][0].isupper(), ent['word'][1:3] == '. ']):\n",
    "            original = ent['word']\n",
    "            ent['word'] = regex.sub(' ', ent['word']).strip()\n",
    "            #corrected.append((original, ent['word']))\n",
    "    return data #, corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_tag_resolution(\n",
    "                          data,\n",
    "                          ent_iter=ent_iter,\n",
    "                          load_pickle=load_pickle,\n",
    "                          persons=KNOWN_PERSONS,\n",
    "                          locations=KNOWN_LOCATIONS,\n",
    "                          organizations=KNOWN_ORGANIZATIONS,\n",
    "                          ):\n",
    "    changes = []\n",
    "    persons = load_pickle(persons)\n",
    "    locations = load_pickle(locations)\n",
    "    organizations = load_pickle(organizations)\n",
    "    print('Resolving tags by database lookup...')\n",
    "    for ent in tqdm(list(ent_iter(data))):\n",
    "        if ent['lemma'] in persons:\n",
    "            changes.append((ent, 'person'))\n",
    "            ent['tag'] = ent['ner'] = 'person'\n",
    "        if ent['lemma'] in locations:\n",
    "            changes.append((ent, 'location'))\n",
    "            ent['tag'] = ent['ner'] = 'location'\n",
    "        if ent['lemma'] in organizations or ent['word'] in organizations:\n",
    "            changes.append((ent, 'organization'))\n",
    "            ent['tag'] = ent['ner'] = 'organization'\n",
    "    save_pickle(r'logs\\lookup_tag_resolution.chge', changes)\n",
    "    return data\n",
    "\n",
    "def lookup_entity_resolution(\n",
    "                             data,\n",
    "                             sentence_iter=sentence_iter,\n",
    "                             generate_entity=generate_entity,\n",
    "                             persons=KNOWN_PERSONS,\n",
    "                             locations=KNOWN_LOCATIONS,\n",
    "                             organizations=KNOWN_ORGANIZATIONS\n",
    "                             ):\n",
    "    changes = []\n",
    "    persons = load_pickle(persons)\n",
    "    locations = load_pickle(locations)\n",
    "    organizations = load_pickle(organizations)\n",
    "    print('Resolving entities by database lookup...')\n",
    "    for sentence in tqdm(list(sentence_iter(data))):\n",
    "        for i, ent in enumerate(sentence):\n",
    "            if not ent['resolved']:\n",
    "                if ent['lemma'] in persons:\n",
    "                    changes.append((ent['word'], persons[ent['lemma']]))\n",
    "                    ent = generate_entity(persons[ent['lemma']], 'person', True)\n",
    "                    sentence[i] = ent\n",
    "                if ent['lemma'] in locations:\n",
    "                    changes.append((ent['word'], locations[ent['lemma']]))\n",
    "                    ent = generate_entity(locations[ent['lemma']], 'location', True)\n",
    "                    sentence[i] = ent\n",
    "                elif ent['word'] in locations:\n",
    "                    changes.append((ent['word'], locations[ent['word']]))\n",
    "                    ent = generate_entity(locations[ent['word']], 'location', True)\n",
    "                    sentence[i] = ent\n",
    "                if ent['lemma'] in organizations:\n",
    "                    changes.append((ent['word'], organizations[ent['lemma']]))\n",
    "                    ent = generate_entity(organizations[ent['lemma']], 'organization', True)\n",
    "                    sentence[i] = ent\n",
    "                elif ent['word'] in organizations:\n",
    "                    changes.append((ent['word'], organizations[ent['word']]))\n",
    "                    ent = generate_entity(organizations[ent['word']], 'organization', True)\n",
    "                    sentence[i] = ent\n",
    "                    \n",
    "                \n",
    "    save_pickle(r'logs\\lookup_entity_resolution.chge', changes)\n",
    "    return data\n",
    "\n",
    "def is_first_or_last_name(ent1, ent2, string=False):\n",
    "    '''\n",
    "    Returns true if ent1 is the first or last word of ent 2\n",
    "    '''\n",
    "    if not string:\n",
    "        ent1 = ent1['word']\n",
    "    return ent2['n_words'] > 1 \\\n",
    "           and any([ent2['words'][0].lower() == ent1.lower(),\n",
    "           ent2['words'][-1].lower() == ent1.lower()])\n",
    "\n",
    "def resolve_hyphenated_words(data, sentence_iter=sentence_iter):\n",
    "    '''\n",
    "    Removes the hyphen in hyphenated words and trys to match each word to some ealier in the document\n",
    "    '''\n",
    "    changes = []\n",
    "    for doc in tqdm(data):\n",
    "        for i, sentence1 in enumerate(doc['sentences']):\n",
    "            to_append = []\n",
    "            for j, ent1 in enumerate(sentence1):\n",
    "                if '-' in ent1['word']:\n",
    "                    resolved_flag = False\n",
    "                    words = [x for x in ent1['word'].split('-') if x.strip()]\n",
    "                    n_words = len(words)\n",
    "                    matches = []\n",
    "                    n_match = 0\n",
    "                    for temp in words:\n",
    "\n",
    "                        temp = temp.strip()\n",
    "                        for ent2 in sentence1[j - 1:: -1]:\n",
    "                            if is_first_or_last_name(temp, ent2, True):\n",
    "                                n_match += 1\n",
    "                                resolved_flag = True\n",
    "                                matches.append(ent2)\n",
    "                                break\n",
    "\n",
    "                        # Iterate through prevous sentences in reverse\n",
    "                        if not resolved_flag:\n",
    "                            for sentence2 in doc['sentences'][i - 1:: -1]:\n",
    "                                for ent2 in sentence2[:: -1]:\n",
    "                                    if ent2['n_words'] > 1 \\\n",
    "                                    and ent2['words'][0].lower() != ent2['words'][-1].lower() \\\n",
    "                                    and is_first_or_last_name(temp, ent2, True):\n",
    "                                        n_match += 1\n",
    "                                        resolved_flag = True\n",
    "                                        matches.append(ent2)\n",
    "                                        break\n",
    "                                if resolved_flag:\n",
    "                                    break\n",
    "                        if n_match == n_words:\n",
    "                            sentence1[j] = matches[0]\n",
    "                            for match in matches[1:]:\n",
    "                                to_append.append(match)\n",
    "                            changes.append(matches)\n",
    "                            break\n",
    "\n",
    "        sentence1.extend(to_append)\n",
    "    save_pickle(r'logs\\resolve_hyphenated_words.chge', changes) \n",
    "    return data\n",
    "                        \n",
    "\n",
    "\n",
    "def reverse_iter_subset_resolution(\n",
    "                                   data,\n",
    "                                   is_first_or_last_name=is_first_or_last_name\n",
    "                                   ):\n",
    "    '''\n",
    "    Resolves single word entities to multi-word entities by finding the most recent\n",
    "    match in the same document\n",
    "    '''\n",
    "    changes = []\n",
    "    print('Checking if entities are substrings of preceeding entities...')\n",
    "    for doc in tqdm(data):\n",
    "        for i, sentence1 in enumerate(doc['sentences']):\n",
    "            for j, ent1 in enumerate(sentence1):\n",
    "                if not ent1['resolved'] and ent1['n_words'] == 1:\n",
    "                    resolved_flag = False\n",
    "                    # Iterate through current sentence in reverse\n",
    "                    for ent2 in sentence1[j - 1:: -1]:\n",
    "                        if is_first_or_last_name(ent1, ent2):\n",
    "                            changes.append((ent1['word'], ent2['word']))\n",
    "                            sentence1[j] = ent2\n",
    "                            resolved_flag = True\n",
    "                            break\n",
    "                            \n",
    "                    # Iterate through prevous sentences in reverse\n",
    "                    if not resolved_flag:\n",
    "                        for sentence2 in doc['sentences'][i - 1:: -1]:\n",
    "                            for ent2 in sentence2[:: -1]:\n",
    "                                #print(ent2)\n",
    "                                if ent2['n_words'] > 1 \\\n",
    "                                and ent2['words'][0].lower() != ent2['words'][-1].lower() \\\n",
    "                                and is_first_or_last_name(ent1, ent2):\n",
    "                                    changes.append((ent1['word'], ent2['word']))\n",
    "                                    sentence1[j] = ent2\n",
    "                                    resolved_flag = True\n",
    "                                    break\n",
    "                            if resolved_flag:\n",
    "                                break\n",
    "    save_pickle(r'logs\\reverse_iter_subset_resolution.chge', changes)\n",
    "    return data\n",
    "\n",
    "def acronym_match(\n",
    "                  acroynm, \n",
    "                  text, \n",
    "                  regex=ACRO_REPLACE_REGEX,\n",
    "                  trans=ACRO_REPLACE_TRANS\n",
    "                  ):\n",
    "    '''\n",
    "    Matches an acronyms to a name\n",
    "    '''\n",
    "    text = regex.sub(lambda x: trans[re.escape(x.group(0))], text)\n",
    "    words = [x.strip() for x in text.split(' ') if x]\n",
    "    n_char = len(acroynm)\n",
    "    return len(words) == n_char and all([acroynm[i] == words[i][0] for i in range(n_char)])\n",
    "\n",
    "def reverse_iter_abbr_resolution(\n",
    "                                 data,\n",
    "                                 acronym_match=acronym_match,\n",
    "                                 isacronym=ACRO_REGEX\n",
    "                                 ):\n",
    "    '''\n",
    "    Resolves single word, acronym non-person entities '''\n",
    "    print('Matching acronym to earlier mentions...')\n",
    "    changes = []\n",
    "    for doc in tqdm(data):\n",
    "        for i, sentence1 in enumerate(doc['sentences']):\n",
    "            for j, ent1 in enumerate(sentence1):\n",
    "                if ent1['tag'] != 'person' and isacronym.fullmatch(ent1['word']): # not person and abbreviation\n",
    "                    resolved_flag = False\n",
    "                    for ent2 in sentence1[j - 1:: -1]:\n",
    "                        if acronym_match(ent1['word'], ent2['word']):\n",
    "                            changes.append((ent1['word'], ent2['word']))\n",
    "                            sentence1[j] = ent2\n",
    "                            resolved_flag = True\n",
    "                            break\n",
    "                            \n",
    "                    # start iterating through prevous sentences in reverse\n",
    "                    if not resolved_flag:\n",
    "                        for sentence2 in doc['sentences'][i - 1:: -1]:\n",
    "                            for ent2 in sentence2[:: -1]:\n",
    "                                if not isacronym.fullmatch(ent2['word']) \\\n",
    "                                and acronym_match(ent1['word'], ent2['word']):\n",
    "                                    changes.append((ent1['word'], ent2['word']))\n",
    "                                    sentence1[j] = ent2\n",
    "                                    resolved_flag = True\n",
    "                                    break\n",
    "                            if resolved_flag:\n",
    "                                break\n",
    "    save_pickle(r'logs\\reverse_iter_abbr_resolution.chge', changes)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_lookup_location_resolution(\n",
    "                                   data, \n",
    "                                   ent_iter=ent_iter,\n",
    "                                   isacroynm=ACRO_REGEX\n",
    "                                   ):\n",
    "    '''\n",
    "    Looks up location entities in the iso pycountry database\n",
    "    '''\n",
    "    print('Resolving locations by database lookup...')\n",
    "    #block_print() # pycountry has verbose output and I could not disable it easily\n",
    "    changes = []\n",
    "    lookup = {}\n",
    "    for ent in tqdm(list(ent_iter(data))):\n",
    "        if not ent['resolved'] and ent['tag'] == 'location':\n",
    "            # Lookup country codes\n",
    "            if ent['word'] in lookup:\n",
    "                ent['word'] = lookup[ent['word']]\n",
    "            else:\n",
    "                if isacroynm.fullmatch(ent['word']) \\\n",
    "                and ent['ner'] in {'country', 'nationality'} \\\n",
    "                and ent['word'] not in {'EU'}:\n",
    "\n",
    "                    if ent['n_words'] == 2:\n",
    "                        match = pycountry.countries.get(alpha_2=ent['word'])\n",
    "                    elif ent['n_words']  == 3:\n",
    "                        match = pycountry.countries.get(alpha_3=ent['word'])\n",
    "\n",
    "                elif ent['ner'] in {'country', 'nationality'}:\n",
    "                    try:\n",
    "                        temp = pycountry.countries.search_fuzzy(ent['word'])[0]\n",
    "                    except LookupError:\n",
    "                        match = None\n",
    "                        pass\n",
    "                else:\n",
    "                    match = None\n",
    "\n",
    "                if match is not None:\n",
    "                    lookup[ent['word']] = match.name\n",
    "                    if ent['word'] != match.name:\n",
    "                        changes.append((ent['word'], match.name))\n",
    "                    ent['word'] = match.name\n",
    "    save_pickle(r'logs\\iso_lookup_location_resolution.chge', changes)\n",
    "    return data              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\06 NER\\temp'\n",
    "\n",
    "OUTPUT_FILENAME = r'C:\\Users\\Simon\\OneDrive - University of Exeter\\__Project__\\07 NED\\temp\\resolved.pkl'\n",
    "\n",
    "ned_pipe = compose(\n",
    "                   curry(save_pickle)(OUTPUT_FILENAME),\n",
    "#                    capitalize_entities,\n",
    "#                    remove_press,\n",
    "#                    resolution_check,\n",
    "#                    lookup_entity_resolution,\n",
    "#                    resolution_check,\n",
    "#                    iso_lookup_location_resolution,\n",
    "#                    resolution_check,\n",
    "#                    lookup_tag_resolution,\n",
    "#                    resolution_check,\n",
    "#                    reverse_iter_abbr_resolution,\n",
    "#                    resolution_check,\n",
    "#                    reverse_iter_subset_resolution,\n",
    "#                    resolution_check,\n",
    "#                    capitalize_entities,\n",
    "#                    resolution_check,\n",
    "#                    resolve_multi_locations,\n",
    "#                    resolve_s_endings,\n",
    "#                    resolution_check,\n",
    "#                    remove_leading_and_trailing_stops,\n",
    "                   remove_press,\n",
    "                   resolution_check,\n",
    "                   resolve_multi_locations,\n",
    "                   resolve_s_endings,\n",
    "                   resolution_check,\n",
    "                   remove_leading_and_trailing_stops,\n",
    "                   capitalize_entities,\n",
    "                   filter_empty,\n",
    "                   resolution_check,\n",
    "                   lookup_entity_resolution,\n",
    "                   resolution_check,\n",
    "                   iso_lookup_location_resolution,\n",
    "                   resolution_check,\n",
    "                   lookup_tag_resolution,\n",
    "                   resolution_check,\n",
    "                   reverse_iter_abbr_resolution,\n",
    "                   resolution_check,\n",
    "                   reverse_iter_subset_resolution,\n",
    "                   resolution_check,\n",
    "                   manual_resolution,\n",
    "                   resolution_check,\n",
    "                   resolve_double_orgs,\n",
    "                   remove_leading_and_trailing_stops,\n",
    "                   filter_empty,\n",
    "                   resolve_hyphenated_words,\n",
    "                   resolution_check,\n",
    "                   #capitalize_entities,\n",
    "                   analyze_entities,\n",
    "                   filter_publications,\n",
    "                   filter_empty,\n",
    "                   entity_correction,\n",
    "                   remove_titles, # i.e. Mr. Mrs.\n",
    "                   stopword_filter,\n",
    "                   lookup_tag_resolution,\n",
    "                   analyze_entities,\n",
    "                   tag_correction,\n",
    "                   list_data_structure,\n",
    "                   curry(map)(load_pickle),\n",
    "                   curry(directory_explorer)('.pkl'),\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ned_pipe(INPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pickle(OUTPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pickle(r'logs\\reverse_iter_subset_resolution.chge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pickle(r'logs\\reverse_iter_abbr_resolution.chge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP37",
   "language": "python",
   "name": "nlp37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
